{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90a55851",
   "metadata": {},
   "source": [
    "# Dự án NLP: Phân loại Văn bản Tiếng Việt\n",
    "\n",
    "Notebook này triển khai quy trình hoàn chỉnh để phân loại tin tức tiếng Việt thành 20 chủ đề khác nhau.\n",
    "\n",
    "**Quy trình thực hiện:**\n",
    "1.  **Tải & Tiền xử lý dữ liệu:** Chuẩn hóa Unicode, tách từ (tokenization), loại bỏ từ dừng (stopwords).\n",
    "2.  **Mô hình Machine Learning cơ bản:** Naive Bayes, Logistic Regression, SVM (LinearSVC).\n",
    "3.  **Mô hình Deep Learning:** LSTM (Long Short-Term Memory).\n",
    "4.  **Mô hình Deep Learning 2:** TextCNN (Convolutional Neural Networks).\n",
    "5.  **Đánh giá & Dự đoán:** Xuất báo cáo hiệu năng và hệ thống dự đoán thời gian thực."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5612a6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thiết bị đang sử dụng: cuda\n"
     ]
    }
   ],
   "source": [
    "# 1. THIẾT LẬP MÔI TRƯỜNG & THƯ VIỆN\n",
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "import shutil\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "# PyTorch & Transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import scipy.sparse\n",
    "\n",
    "# Matplotlib & Seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Thư viện xử lý tiếng Việt\n",
    "from pyvi import ViTokenizer\n",
    "\n",
    "# Cấu hình đường dẫn\n",
    "CURRENT_DIR = Path.cwd()\n",
    "PROJECT_ROOT = CURRENT_DIR if (CURRENT_DIR / \"data\").exists() else CURRENT_DIR.parent\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"final\"\n",
    "MODEL_DIR = PROJECT_ROOT / \"models\"\n",
    "REPORT_DIR = PROJECT_ROOT / \"reports\"\n",
    "JSONL_PATH = DATA_DIR / \"nlp_dataset.jsonl\"\n",
    "\n",
    "# Tạo thư mục nếu chưa tồn tại\n",
    "for d in [MODEL_DIR, REPORT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Thiết bị đang sử dụng: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02b2f6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đã tải 1942 từ dừng.\n",
      "Đang tải dữ liệu từ file JSONL...\n",
      "Đang xử lý tách từ và lọc stopwords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xử lý văn bản: 100%|██████████| 115188/115188 [08:16<00:00, 231.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng Train: 92150 | Số lượng Test: 23038\n",
      "Danh sách 20 nhãn: ['Bất động sản' 'Chứng khoán' 'Công nghệ' 'Du lịch' 'Gia đình'\n",
      " 'Giao thông' 'Giáo dục' 'Giải trí' 'Khoa học' 'Khởi nghiệp' 'Kinh doanh'\n",
      " 'Nông nghiệp' 'Pháp luật' 'Sức khỏe' 'Thế giới' 'Thể thao'\n",
      " 'Thời sự – Chính trị' 'Văn hóa' 'Đời sống' 'Ẩm thực']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. TẢI VÀ TIỀN XỬ LÝ DỮ LIỆU\n",
    "\n",
    "STOPWORD_PATH = PROJECT_ROOT / \"data\" / \"final\" / \"vietnamese-stopwords-dash.txt\"\n",
    "\n",
    "def load_stopwords(filepath):\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            return set([line.strip() for line in f.readlines()])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Lỗi: Không tìm thấy file stopwords tại {filepath}\")\n",
    "        return {\"thì\", \"là\", \"mà\"}\n",
    "\n",
    "STOPWORDS = load_stopwords(STOPWORD_PATH)\n",
    "print(f\"Đã tải {len(STOPWORDS)} từ dừng.\")\n",
    "\n",
    "def normalize_text(text):\n",
    "    return unicodedata.normalize('NFC', text)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = normalize_text(text)\n",
    "    tokenized = ViTokenizer.tokenize(text)\n",
    "    words = tokenized.split()\n",
    "    clean_words = [w for w in words if w.lower() not in STOPWORDS]\n",
    "    return \" \".join(clean_words)\n",
    "\n",
    "if JSONL_PATH.exists():\n",
    "    print(\"Đang tải dữ liệu từ file JSONL...\")\n",
    "    df = pd.read_json(JSONL_PATH, lines=True)\n",
    "    \n",
    "    if 'raw_text' not in df.columns:\n",
    "        df['raw_text'] = df['text'].apply(normalize_text)\n",
    "    \n",
    "    print(\"Đang xử lý tách từ và lọc stopwords...\")\n",
    "    tqdm.pandas(desc=\"Xử lý văn bản\")\n",
    "    df['text'] = df['raw_text'].progress_apply(preprocess_text)\n",
    "    \n",
    "    df.to_json(JSONL_PATH, orient=\"records\", lines=True)\n",
    "else:\n",
    "    print(\"Lỗi: Không tìm thấy file dữ liệu.\")\n",
    "\n",
    "target_col = 'label_name'\n",
    "if target_col not in df.columns and 'label' in df.columns:\n",
    "    df[target_col] = df['label']\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['label_id'] = le.fit_transform(df[target_col])\n",
    "classes = le.classes_\n",
    "num_classes = len(classes)\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[target_col])\n",
    "\n",
    "print(f\"Số lượng Train: {len(train_df)} | Số lượng Test: {len(test_df)}\")\n",
    "print(f\"Danh sách {num_classes} nhãn: {classes}\")\n",
    "\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67524436",
   "metadata": {},
   "source": [
    "# 3. MÔ HÌNH MACHINE LEARNING CƠ BẢN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c560ad97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang tạo vector TF-IDF...\n",
      "Đang huấn luyện Naive Bayes...\n",
      "Naive Bayes Accuracy: 0.8264\n",
      "Đang huấn luyện Logistic Regression...\n",
      "Logistic Regression Accuracy: 0.8769\n",
      "Đang huấn luyện SVM (LinearSVC) với chế độ chuẩn hóa xác suất...\n",
      "SVM (Calibrated) Accuracy: 0.8831\n"
     ]
    }
   ],
   "source": [
    "# Tạo đặc trưng TF-IDF\n",
    "print(\"Đang tạo vector TF-IDF...\")\n",
    "tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1, 2))\n",
    "X_train = tfidf.fit_transform(train_df['text'])\n",
    "X_test = tfidf.transform(test_df['text'])\n",
    "\n",
    "# 1. Naive Bayes\n",
    "print(\"Đang huấn luyện Naive Bayes...\")\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, train_df['label_id'])\n",
    "acc_nb = accuracy_score(test_df['label_id'], nb.predict(X_test))\n",
    "print(f\"Naive Bayes Accuracy: {acc_nb:.4f}\")\n",
    "\n",
    "# 2. Logistic Regression\n",
    "print(\"Đang huấn luyện Logistic Regression...\")\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
    "lr.fit(X_train, train_df['label_id'])\n",
    "acc_lr = accuracy_score(test_df['label_id'], lr.predict(X_test))\n",
    "print(f\"Logistic Regression Accuracy: {acc_lr:.4f}\")\n",
    "\n",
    "# 3. SVM (LinearSVC) \n",
    "print(\"Đang huấn luyện SVM (LinearSVC) với chế độ chuẩn hóa xác suất...\")\n",
    "linear_svc = LinearSVC(dual=False, random_state=42, max_iter=1000)\n",
    "svm = CalibratedClassifierCV(linear_svc, method='sigmoid', cv=5) \n",
    "svm.fit(X_train, train_df['label_id'])\n",
    "acc_svm = accuracy_score(test_df['label_id'], svm.predict(X_test))\n",
    "print(f\"SVM (Calibrated) Accuracy: {acc_svm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bc8470",
   "metadata": {},
   "source": [
    "# 4. DEEP LEARNING (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cb8e86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Dimension: 20000\n",
      "Number of Classes: 20\n",
      "Device: cuda\n",
      "Training LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LSTM Epoch 1: 100%|██████████| 1440/1440 [00:15<00:00, 90.21it/s, loss=0.4073]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> Val Accuracy: 0.8746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LSTM Epoch 2: 100%|██████████| 1440/1440 [00:15<00:00, 95.13it/s, loss=0.1557]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> Val Accuracy: 0.8792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LSTM Epoch 3: 100%|██████████| 1440/1440 [00:15<00:00, 95.34it/s, loss=0.1526]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> Val Accuracy: 0.8692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LSTM Epoch 4: 100%|██████████| 1440/1440 [00:14<00:00, 96.35it/s, loss=0.1094]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> Val Accuracy: 0.8687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LSTM Epoch 5: 100%|██████████| 1440/1440 [00:15<00:00, 95.49it/s, loss=0.1414]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> Val Accuracy: 0.8655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LSTM Epoch 6: 100%|██████████| 1440/1440 [00:14<00:00, 96.01it/s, loss=0.0723]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> Val Accuracy: 0.8640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LSTM Epoch 7: 100%|██████████| 1440/1440 [00:14<00:00, 96.25it/s, loss=0.0382]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> Val Accuracy: 0.8620\n",
      "Early Stopping triggered for LSTM.\n",
      "Best LSTM Accuracy: 0.8792\n"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = X_train.shape[1]\n",
    "NUM_CLASSES = len(le.classes_)\n",
    "print(f\"Input Dimension: {INPUT_DIM}\")\n",
    "print(f\"Number of Classes: {NUM_CLASSES}\")\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "class SparseDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if scipy.sparse.issparse(self.X):\n",
    "            row = self.X[idx].toarray().squeeze()\n",
    "        else:\n",
    "            row = self.X[idx]\n",
    "        return torch.tensor(row, dtype=torch.float32), self.y[idx]\n",
    "\n",
    "y_train_vals = train_df['label_id'].values\n",
    "y_test_vals = test_df['label_id'].values\n",
    "\n",
    "train_dataset = SparseDataset(X_train, y_train_vals)\n",
    "test_dataset = SparseDataset(X_test, y_test_vals)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# --- LSTM MODEL ---\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, num_layers=2, batch_first=True, dropout=0.3)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = lstm_out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "model_lstm = LSTM(input_size=INPUT_DIM, hidden_dim=256, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_lstm.parameters(), lr=0.001)\n",
    "\n",
    "# Early Stopping Config for LSTM\n",
    "best_acc_lstm = 0.0\n",
    "patience_lstm = 5\n",
    "counter_lstm = 0\n",
    "best_lstm_wts = copy.deepcopy(model_lstm.state_dict())\n",
    "\n",
    "print(\"Training LSTM\")\n",
    "for epoch in range(15): \n",
    "    model_lstm.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(train_loader, desc=f\"LSTM Epoch {epoch+1}\")\n",
    "    \n",
    "    for x_batch, y_batch in pbar:\n",
    "        x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model_lstm(x_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "\n",
    "    model_lstm.eval()\n",
    "    preds_temp = []\n",
    "    labels_temp = []\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in test_loader:\n",
    "            x_val = x_val.to(DEVICE)\n",
    "            out = model_lstm(x_val)\n",
    "            preds_temp.extend(torch.argmax(out, dim=1).cpu().numpy())\n",
    "            labels_temp.extend(y_val.numpy())\n",
    "    \n",
    "    val_acc = accuracy_score(labels_temp, preds_temp)\n",
    "    print(f\" >> Val Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    if val_acc > best_acc_lstm:\n",
    "        best_acc_lstm = val_acc\n",
    "        best_lstm_wts = copy.deepcopy(model_lstm.state_dict())\n",
    "        counter_lstm = 0\n",
    "    else:\n",
    "        counter_lstm += 1\n",
    "        if counter_lstm >= patience_lstm:\n",
    "            print(\"Early Stopping triggered for LSTM.\")\n",
    "            break\n",
    "\n",
    "model_lstm.load_state_dict(best_lstm_wts)\n",
    "print(f\"Best LSTM Accuracy: {best_acc_lstm:.4f}\")\n",
    "\n",
    "model_lstm.eval()\n",
    "preds_lstm = []\n",
    "with torch.no_grad():\n",
    "    for x_batch, _ in test_loader:\n",
    "        x_batch = x_batch.to(DEVICE)\n",
    "        out = model_lstm(x_batch)\n",
    "        preds_lstm.extend(torch.argmax(out, dim=1).cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ed01d2",
   "metadata": {},
   "source": [
    "# 5. DEEP LEARNING MODEL 2: TextMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b62d8546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MLP (Input Dim: 20000)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 1440/1440 [00:13<00:00, 103.01it/s, loss=0.6654]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> Val Accuracy: 0.8706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 1440/1440 [00:14<00:00, 102.36it/s, loss=0.5114]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> Val Accuracy: 0.8733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 1440/1440 [00:13<00:00, 104.49it/s, loss=0.4761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> Val Accuracy: 0.8764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 1440/1440 [00:14<00:00, 102.14it/s, loss=0.2524]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> Val Accuracy: 0.8741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 1440/1440 [00:14<00:00, 99.14it/s, loss=0.5309] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> Val Accuracy: 0.8760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 1440/1440 [00:14<00:00, 99.69it/s, loss=0.3587] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> Val Accuracy: 0.8752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 1440/1440 [00:14<00:00, 100.46it/s, loss=0.2540]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> Val Accuracy: 0.8749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 1440/1440 [00:14<00:00, 98.45it/s, loss=0.2201] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> Val Accuracy: 0.8740\n",
      "Early Stopping.\n",
      "Best MLP Accuracy: 0.8764\n"
     ]
    }
   ],
   "source": [
    "class TextMLP(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(TextMLP, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        \n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.out = nn.Linear(64, num_classes)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.bn1(self.fc1(x))))\n",
    "        x = self.dropout(F.relu(self.bn2(self.fc2(x))))\n",
    "        x = self.dropout(F.relu(self.bn3(self.fc3(x))))\n",
    "        return self.out(x)\n",
    "\n",
    "# --- 3. HUẤN LUYỆN ---\n",
    "model_mlp = TextMLP(input_size=INPUT_DIM, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_mlp.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "\n",
    "best_acc = 0.0\n",
    "patience = 5\n",
    "counter = 0\n",
    "best_weights = copy.deepcopy(model_mlp.state_dict())\n",
    "\n",
    "print(f\"Training MLP (Input Dim: {INPUT_DIM})...\")\n",
    "\n",
    "for epoch in range(20):\n",
    "    model_mlp.train()\n",
    "    total_loss = 0\n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
    "    \n",
    "    for x, y in pbar:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model_mlp(x)\n",
    "        loss = criterion(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "    \n",
    "    model_mlp.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(DEVICE)\n",
    "            output = model_mlp(x)\n",
    "            preds.extend(torch.argmax(output, dim=1).cpu().numpy())\n",
    "            labels.extend(y.numpy())\n",
    "            \n",
    "    val_acc = accuracy_score(labels, preds)\n",
    "    print(f\" >> Val Accuracy: {val_acc:.4f}\")\n",
    "    \n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_weights = copy.deepcopy(model_mlp.state_dict())\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early Stopping.\")\n",
    "            break\n",
    "\n",
    "model_mlp.load_state_dict(best_weights)\n",
    "print(f\"Best MLP Accuracy: {best_acc:.4f}\")\n",
    "\n",
    "# Lưu biến dự đoán để dùng cho báo cáo sau này\n",
    "preds_mlp = []\n",
    "model_mlp.eval()\n",
    "with torch.no_grad():\n",
    "    for x, _ in test_loader:\n",
    "        x = x.to(DEVICE)\n",
    "        output = model_mlp(x)\n",
    "        preds_mlp.extend(torch.argmax(output, dim=1).cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b0bfb1",
   "metadata": {},
   "source": [
    "# 6. ĐÁNH GIÁ & BÁO CÁO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12b924cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dang thu thap ket qua du doan...\n",
      "Dang xuat Excel report: detailed_classification_report.xlsx\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "At least one sheet must be visible",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 48\u001b[39m\n\u001b[32m     47\u001b[39m acc = accuracy_score(test_df[\u001b[33m'\u001b[39m\u001b[33mlabel_id\u001b[39m\u001b[33m'\u001b[39m], y_pred)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m precision, recall, f1, _ = \u001b[43mprecision_recall_fscore_support\u001b[49m(test_df[\u001b[33m'\u001b[39m\u001b[33mlabel_id\u001b[39m\u001b[33m'\u001b[39m], y_pred, average=\u001b[33m'\u001b[39m\u001b[33mweighted\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     50\u001b[39m summary_data.append({\n\u001b[32m     51\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mModel\u001b[39m\u001b[33m\"\u001b[39m: model_name,\n\u001b[32m     52\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAccuracy\u001b[39m\u001b[33m\"\u001b[39m: acc,\n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mF1-Score\u001b[39m\u001b[33m\"\u001b[39m: f1\n\u001b[32m     56\u001b[39m })\n",
      "\u001b[31mNameError\u001b[39m: name 'precision_recall_fscore_support' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     37\u001b[39m report_path = REPORT_DIR / \u001b[33m\"\u001b[39m\u001b[33mdetailed_classification_report.xlsx\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDang xuat Excel report: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreport_path.name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pd.ExcelWriter(report_path) \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[32m     41\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m model_name, y_pred \u001b[38;5;129;01min\u001b[39;00m model_preds.items():\n\u001b[32m     42\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_pred) != \u001b[38;5;28mlen\u001b[39m(test_df):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\trading\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1353\u001b[39m, in \u001b[36mExcelWriter.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_value, traceback)\u001b[39m\n\u001b[32m   1347\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\n\u001b[32m   1348\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1349\u001b[39m     exc_type: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;167;01mBaseException\u001b[39;00m] | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1350\u001b[39m     exc_value: \u001b[38;5;167;01mBaseException\u001b[39;00m | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1351\u001b[39m     traceback: TracebackType | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1352\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1353\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\trading\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1357\u001b[39m, in \u001b[36mExcelWriter.close\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1355\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclose\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1356\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"synonym for save, to make it more file-like\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1357\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1358\u001b[39m     \u001b[38;5;28mself\u001b[39m._handles.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\trading\\Lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:110\u001b[39m, in \u001b[36mOpenpyxlWriter._save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_save\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    107\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[33;03m    Save workbook to disk.\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbook\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handles\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr+\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m._handles.handle, mmap.mmap):\n\u001b[32m    112\u001b[39m         \u001b[38;5;66;03m# truncate file to the written content\u001b[39;00m\n\u001b[32m    113\u001b[39m         \u001b[38;5;28mself\u001b[39m._handles.handle.truncate()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\trading\\Lib\\site-packages\\openpyxl\\workbook\\workbook.py:386\u001b[39m, in \u001b[36mWorkbook.save\u001b[39m\u001b[34m(self, filename)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.write_only \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.worksheets:\n\u001b[32m    385\u001b[39m     \u001b[38;5;28mself\u001b[39m.create_sheet()\n\u001b[32m--> \u001b[39m\u001b[32m386\u001b[39m \u001b[43msave_workbook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\trading\\Lib\\site-packages\\openpyxl\\writer\\excel.py:294\u001b[39m, in \u001b[36msave_workbook\u001b[39m\u001b[34m(workbook, filename)\u001b[39m\n\u001b[32m    292\u001b[39m workbook.properties.modified = datetime.datetime.now(tz=datetime.timezone.utc).replace(tzinfo=\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    293\u001b[39m writer = ExcelWriter(workbook, archive)\n\u001b[32m--> \u001b[39m\u001b[32m294\u001b[39m \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    295\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\trading\\Lib\\site-packages\\openpyxl\\writer\\excel.py:275\u001b[39m, in \u001b[36mExcelWriter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msave\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    274\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Write data into the archive.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m     \u001b[38;5;28mself\u001b[39m._archive.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\trading\\Lib\\site-packages\\openpyxl\\writer\\excel.py:89\u001b[39m, in \u001b[36mExcelWriter.write_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     87\u001b[39m writer = WorkbookWriter(\u001b[38;5;28mself\u001b[39m.workbook)\n\u001b[32m     88\u001b[39m archive.writestr(ARC_ROOT_RELS, writer.write_root_rels())\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m archive.writestr(ARC_WORKBOOK, \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     90\u001b[39m archive.writestr(ARC_WORKBOOK_RELS, writer.write_rels())\n\u001b[32m     92\u001b[39m \u001b[38;5;28mself\u001b[39m._merge_vba()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\trading\\Lib\\site-packages\\openpyxl\\workbook\\_writer.py:150\u001b[39m, in \u001b[36mWorkbookWriter.write\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    148\u001b[39m \u001b[38;5;28mself\u001b[39m.write_names()\n\u001b[32m    149\u001b[39m \u001b[38;5;28mself\u001b[39m.write_pivots()\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwrite_views\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28mself\u001b[39m.write_refs()\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tostring(\u001b[38;5;28mself\u001b[39m.package.to_tree())\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\trading\\Lib\\site-packages\\openpyxl\\workbook\\_writer.py:137\u001b[39m, in \u001b[36mWorkbookWriter.write_views\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrite_views\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     active = \u001b[43mget_active_sheet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.wb.views:\n\u001b[32m    139\u001b[39m         \u001b[38;5;28mself\u001b[39m.wb.views[\u001b[32m0\u001b[39m].activeTab = active\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\trading\\Lib\\site-packages\\openpyxl\\workbook\\_writer.py:35\u001b[39m, in \u001b[36mget_active_sheet\u001b[39m\u001b[34m(wb)\u001b[39m\n\u001b[32m     33\u001b[39m visible_sheets = [idx \u001b[38;5;28;01mfor\u001b[39;00m idx, sheet \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(wb._sheets) \u001b[38;5;28;01mif\u001b[39;00m sheet.sheet_state == \u001b[33m\"\u001b[39m\u001b[33mvisible\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m visible_sheets:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAt least one sheet must be visible\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     37\u001b[39m idx = wb._active_sheet_index\n\u001b[32m     38\u001b[39m sheet = wb.active\n",
      "\u001b[31mIndexError\u001b[39m: At least one sheet must be visible"
     ]
    }
   ],
   "source": [
    "print(\"Dang thu thap ket qua du doan...\")\n",
    "model_preds = {}\n",
    "\n",
    "def get_ml_pred(model_var_name, file_name, x_input):\n",
    "    model = None\n",
    "    if model_var_name in globals() and globals()[model_var_name] is not None:\n",
    "        model = globals()[model_var_name]\n",
    "    elif (MODEL_DIR / file_name).exists():\n",
    "        try:\n",
    "            print(f\"Dang load lai {model_var_name} tu file...\")\n",
    "            model = joblib.load(MODEL_DIR / file_name)\n",
    "        except: pass\n",
    "    \n",
    "    if model is not None:\n",
    "        try:\n",
    "            return model.predict(x_input)\n",
    "        except Exception as e:\n",
    "            print(f\"Loi du doan {model_var_name}: {e}\")\n",
    "    return None\n",
    "\n",
    "pred_nb = get_ml_pred('nb', 'naive_bayes.pkl', X_test)\n",
    "if pred_nb is not None: model_preds['Naive Bayes'] = pred_nb\n",
    "\n",
    "pred_lr = get_ml_pred('lr', 'logistic_regression.pkl', X_test)\n",
    "if pred_lr is not None: model_preds['Logistic Regression'] = pred_lr\n",
    "\n",
    "pred_svm = get_ml_pred('svm', 'svm_linear.pkl', X_test)\n",
    "if pred_svm is not None: model_preds['SVM'] = pred_svm\n",
    "\n",
    "if 'preds_lstm' in globals():\n",
    "    model_preds['LSTM'] = np.array(preds_lstm)\n",
    "\n",
    "if 'preds_mlp' in globals():\n",
    "    model_preds['TextMLP'] = np.array(preds_mlp)\n",
    "\n",
    "summary_data = []\n",
    "report_path = REPORT_DIR / \"detailed_classification_report.xlsx\"\n",
    "print(f\"Dang xuat Excel report: {report_path.name}\")\n",
    "\n",
    "with pd.ExcelWriter(report_path) as writer:\n",
    "    for model_name, y_pred in model_preds.items():\n",
    "        if len(y_pred) != len(test_df):\n",
    "            print(f\"Warning: {model_name} length mismatch. Skipping.\")\n",
    "            continue\n",
    "            \n",
    "        # 1. Tính toán các chỉ số\n",
    "        acc = accuracy_score(test_df['label_id'], y_pred)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(test_df['label_id'], y_pred, average='weighted')\n",
    "        \n",
    "        summary_data.append({\n",
    "            \"Model\": model_name,\n",
    "            \"Accuracy\": acc,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1-Score\": f1\n",
    "        })\n",
    "        \n",
    "        # 2. Lưu report chi tiết từng class vào Excel\n",
    "        clf_report = classification_report(test_df['label_id'], y_pred, target_names=classes, output_dict=True)\n",
    "        df_report = pd.DataFrame(clf_report).transpose()\n",
    "        sheet_name = model_name[:31]\n",
    "        df_report.to_excel(writer, sheet_name=sheet_name)\n",
    "\n",
    "if summary_data:\n",
    "    # 3. HIỂN THỊ BẢNG TỔNG HỢP (Yêu cầu của bạn)\n",
    "    results_df = pd.DataFrame(summary_data).sort_values(by=\"Accuracy\", ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"BANG TONG HOP KET QUA (WEIGHTED AVERAGE)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Hiển thị bảng với định dạng %\n",
    "    display(results_df.style.format({\n",
    "        \"Accuracy\": \"{:.2%}\",\n",
    "        \"Precision\": \"{:.2%}\",\n",
    "        \"Recall\": \"{:.2%}\",\n",
    "        \"F1-Score\": \"{:.2%}\"\n",
    "    }).background_gradient(cmap='Blues'))\n",
    "    \n",
    "    # 4. Vẽ biểu đồ Accuracy\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(data=results_df, x=\"Accuracy\", y=\"Model\", palette=\"viridis\", hue=\"Model\", legend=False)\n",
    "    plt.title(\"So sanh Accuracy giua cac mo hinh\")\n",
    "    plt.xlim(0, 1.15)\n",
    "    plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "    for i, row in results_df.iterrows():\n",
    "        plt.text(row.Accuracy + 0.01, i, f\"{row.Accuracy:.2%}\", va='center', fontweight='bold', color='black')\n",
    "\n",
    "    plt.savefig(REPORT_DIR / \"model_comparison.png\", bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "print(f\"Dang ve Confusion Matrix cho {len(model_preds)} mo hinh...\")\n",
    "\n",
    "for model_name, y_pred in model_preds.items():\n",
    "    if len(y_pred) != len(test_df): continue\n",
    "\n",
    "    cm = confusion_matrix(test_df['label_id'], y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    color_map = 'Blues'\n",
    "    if 'SVM' in model_name: color_map = 'Greens'\n",
    "    if 'TextMLP' in model_name: color_map = 'Purples'\n",
    "    if 'LSTM' in model_name: color_map = 'Oranges'\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=color_map, \n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    \n",
    "    plt.title(f\"Confusion Matrix - {model_name}\", fontsize=15, fontweight='bold', pad=20)\n",
    "    plt.ylabel('Nhan thuc te')\n",
    "    plt.xlabel('Nhan du doan')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    safe_name = model_name.replace(\" \", \"_\")\n",
    "    plt.savefig(REPORT_DIR / f\"confusion_matrix_{safe_name}.png\")\n",
    "    plt.close()\n",
    "\n",
    "print(\"Dang luu cac mo hinh...\")\n",
    "\n",
    "if 'le' in globals(): joblib.dump(le, MODEL_DIR / \"label_encoder.pkl\")\n",
    "if 'vectorizer' in globals(): joblib.dump(vectorizer, MODEL_DIR / \"tfidf_vectorizer_dl.pkl\") \n",
    "elif 'tfidf' in globals(): joblib.dump(tfidf, MODEL_DIR / \"tfidf_vectorizer.pkl\")\n",
    "\n",
    "if 'nb' in globals() and nb is not None: joblib.dump(nb, MODEL_DIR / \"naive_bayes.pkl\")\n",
    "if 'lr' in globals() and lr is not None: joblib.dump(lr, MODEL_DIR / \"logistic_regression.pkl\")\n",
    "if 'svm' in globals() and svm is not None: joblib.dump(svm, MODEL_DIR / \"svm_linear.pkl\")\n",
    "\n",
    "if 'model_lstm' in globals():\n",
    "    torch.save(model_lstm.state_dict(), MODEL_DIR / \"lstm_model.pth\")\n",
    "    print(\" - Da luu LSTM Model (lstm_model.pth).\")\n",
    "\n",
    "if 'model_mlp' in globals():\n",
    "    torch.save(model_mlp.state_dict(), MODEL_DIR / \"mlp_model.pth\")\n",
    "    print(\" - Da luu TextMLP Model (mlp_model.pth).\")\n",
    "\n",
    "print(\"Hoan tat toan bo quy trinh!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abc1f78",
   "metadata": {},
   "source": [
    "# 7. HỆ THỐNG DỰ ĐOÁN THỰC TẾ (INFERENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a476734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "Input: https://vnexpress.net/nu-sinh-gianh-hc-vang-sea-games-duoc-t...\n",
      "---------------------------------------------------------------------------\n",
      "MODEL                | LABEL                               | CONF\n",
      "---------------------------------------------------------------------------\n",
      "Naive Bayes          | GIÁO DỤC                            | 48.52%\n",
      "Logistic Reg         | GIÁO DỤC                            | 48.32%\n",
      "SVM                  | GIÁO DỤC                            | 52.42%\n",
      "LSTM                 | GIÁO DỤC                            | 82.24%\n",
      "TextMLP              | GIÁO DỤC                            | 91.67%\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import joblib\n",
    "import torch.nn.functional as F\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from pyvi import ViTokenizer\n",
    "\n",
    "CURRENT_DIR = Path.cwd()\n",
    "PROJECT_ROOT = CURRENT_DIR if (CURRENT_DIR / \"data\").exists() else CURRENT_DIR.parent\n",
    "MODEL_DIR = PROJECT_ROOT / \"models\"\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# --- 1. ĐỊNH NGHĨA MODEL ---\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dim, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, num_layers=2, batch_first=True, dropout=0.3)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = lstm_out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "class TextMLP(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(TextMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.bn3 = nn.BatchNorm1d(64)\n",
    "        self.out = nn.Linear(64, num_classes)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(F.relu(self.bn1(self.fc1(x))))\n",
    "        x = self.dropout(F.relu(self.bn2(self.fc2(x))))\n",
    "        x = self.dropout(F.relu(self.bn3(self.fc3(x))))\n",
    "        return self.out(x)\n",
    "\n",
    "# --- 2. LOAD MODELS & VECTORIZER ---\n",
    "le, tfidf = None, None\n",
    "nb, lr, svm = None, None, None\n",
    "lstm_model, mlp_model = None, None\n",
    "\n",
    "try:\n",
    "    # Load Label Encoder & Vectorizer\n",
    "    if (MODEL_DIR / \"label_encoder.pkl\").exists(): \n",
    "        le = joblib.load(MODEL_DIR / \"label_encoder.pkl\")\n",
    "    elif (MODEL_DIR / \"le.pkl\").exists():\n",
    "        le = joblib.load(MODEL_DIR / \"le.pkl\")\n",
    "    \n",
    "    if (MODEL_DIR / \"tfidf_vectorizer_dl.pkl\").exists():\n",
    "        tfidf = joblib.load(MODEL_DIR / \"tfidf_vectorizer_dl.pkl\")\n",
    "    elif (MODEL_DIR / \"tfidf_vectorizer.pkl\").exists():\n",
    "        tfidf = joblib.load(MODEL_DIR / \"tfidf_vectorizer.pkl\")\n",
    "    \n",
    "    # Load ML Models\n",
    "    if (MODEL_DIR / \"naive_bayes.pkl\").exists(): nb = joblib.load(MODEL_DIR / \"naive_bayes.pkl\")\n",
    "    if (MODEL_DIR / \"logistic_regression.pkl\").exists(): lr = joblib.load(MODEL_DIR / \"logistic_regression.pkl\")\n",
    "    if (MODEL_DIR / \"svm_linear.pkl\").exists(): svm = joblib.load(MODEL_DIR / \"svm_linear.pkl\")\n",
    "\n",
    "    if tfidf and le:\n",
    "        input_dim = len(tfidf.vocabulary_)\n",
    "        num_classes = len(le.classes_)\n",
    "\n",
    "        # Load LSTM (TF-IDF Version)\n",
    "        if (MODEL_DIR / \"lstm_model.pth\").exists():\n",
    "            lstm_model = LSTM(input_size=input_dim, hidden_dim=256, num_classes=num_classes)\n",
    "            lstm_model.load_state_dict(torch.load(MODEL_DIR / \"lstm_model.pth\", map_location=DEVICE))\n",
    "            lstm_model.to(DEVICE).eval()\n",
    "\n",
    "        # Load TextMLP (Thay thế TextCNN)\n",
    "        if (MODEL_DIR / \"mlp_model.pth\").exists():\n",
    "            mlp_model = TextMLP(input_size=input_dim, num_classes=num_classes)\n",
    "            mlp_model.load_state_dict(torch.load(MODEL_DIR / \"mlp_model.pth\", map_location=DEVICE))\n",
    "            mlp_model.to(DEVICE).eval()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Load Error: {e}\")\n",
    "\n",
    "# --- 3. HÀM DỰ ĐOÁN ---\n",
    "def predict_all_models(url_or_text):\n",
    "    print(f\"\\nInput: {url_or_text[:60]}...\")\n",
    "    \n",
    "    content = url_or_text\n",
    "    if url_or_text.startswith(\"http\"):\n",
    "        try:\n",
    "            headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "            resp = requests.get(url_or_text, headers=headers, timeout=10)\n",
    "            soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "            content = ' '.join([p.get_text() for p in soup.find_all('p')]) \n",
    "            if len(content) < 50: return print(\"Short content.\")\n",
    "        except Exception as e: return print(f\"URL Error: {e}\")\n",
    "\n",
    "    text_seg = ViTokenizer.tokenize(content)\n",
    "    \n",
    "    if not tfidf:\n",
    "        return print(\"TF-IDF Vectorizer missing.\")\n",
    "\n",
    "    vec_sparse = tfidf.transform([text_seg]) \n",
    "    \n",
    "    print(\"-\" * 75)\n",
    "    print(f\"{'MODEL':<20} | {'LABEL':<35} | {'CONF'}\")\n",
    "    print(\"-\" * 75)\n",
    "\n",
    "    if le:\n",
    "        # ML Predictions\n",
    "        if nb:\n",
    "            print(f\"{'Naive Bayes':<20} | {le.inverse_transform(nb.predict(vec_sparse))[0].upper():<35} | {nb.predict_proba(vec_sparse).max():.2%}\")\n",
    "        if lr:\n",
    "            print(f\"{'Logistic Reg':<20} | {le.inverse_transform(lr.predict(vec_sparse))[0].upper():<35} | {lr.predict_proba(vec_sparse).max():.2%}\")\n",
    "        if svm:\n",
    "            try: p_str = f\"{svm.predict_proba(vec_sparse).max():.2%}\"\n",
    "            except: p_str = \"N/A\"\n",
    "            print(f\"{'SVM':<20} | {le.inverse_transform(svm.predict(vec_sparse))[0].upper():<35} | {p_str}\")\n",
    "\n",
    "        # Deep Learning Predictions (Input là Dense Tensor)\n",
    "        vec_dense = torch.tensor(vec_sparse.toarray(), dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "        if lstm_model:\n",
    "            with torch.no_grad():\n",
    "                out = lstm_model(vec_dense)\n",
    "                prob, idx = torch.max(torch.softmax(out, dim=1), dim=1)\n",
    "                print(f\"{'LSTM':<20} | {le.inverse_transform([idx.item()])[0].upper():<35} | {prob.item():.2%}\")\n",
    "        \n",
    "        if mlp_model:\n",
    "            with torch.no_grad():\n",
    "                out = mlp_model(vec_dense)\n",
    "                prob, idx = torch.max(torch.softmax(out, dim=1), dim=1)\n",
    "                print(f\"{'TextMLP':<20} | {le.inverse_transform([idx.item()])[0].upper():<35} | {prob.item():.2%}\")\n",
    "    \n",
    "    print(\"-\" * 75)\n",
    "\n",
    "# --- 4. CHẠY THỬ ---\n",
    "link_test = \"https://vnexpress.net/nu-sinh-gianh-hc-vang-sea-games-duoc-truong-thuong-hon-100-trieu-dong-4996452.html\"\n",
    "predict_all_models(link_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (trading)",
   "language": "python",
   "name": "trading"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
