{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90a55851",
   "metadata": {},
   "source": [
    "# Dự án NLP: Phân loại Văn bản Tiếng Việt\n",
    "\n",
    "Notebook này triển khai quy trình hoàn chỉnh để phân loại tin tức tiếng Việt thành 20 chủ đề khác nhau.\n",
    "\n",
    "**Quy trình thực hiện:**\n",
    "1.  **Tải & Tiền xử lý dữ liệu:** Chuẩn hóa Unicode, tách từ (tokenization), loại bỏ từ dừng (stopwords).\n",
    "2.  **Mô hình Machine Learning cơ bản:** Naive Bayes, Logistic Regression, SVM (LinearSVC), SGD Classifier.\n",
    "3.  **Mô hình Deep Learning:** LSTM (Long Short-Term Memory).\n",
    "4.  **Mô hình Transformers:** Fine-tuning PhoBERT (Sử dụng chiến thuật cắt ghép đầu-đuôi).\n",
    "5.  **Đánh giá & Dự đoán:** Xuất báo cáo hiệu năng và hệ thống dự đoán thời gian thực."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5612a6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thiết bị đang sử dụng: cuda\n"
     ]
    }
   ],
   "source": [
    "# 1. THIẾT LẬP MÔI TRƯỜNG & THƯ VIỆN\n",
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "import shutil\n",
    "import unicodedata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch & Transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Scikit-Learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Thư viện xử lý tiếng Việt\n",
    "from pyvi import ViTokenizer\n",
    "\n",
    "# Cấu hình đường dẫn\n",
    "CURRENT_DIR = Path.cwd()\n",
    "PROJECT_ROOT = CURRENT_DIR if (CURRENT_DIR / \"data\").exists() else CURRENT_DIR.parent\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"final\"\n",
    "MODEL_DIR = PROJECT_ROOT / \"models\"\n",
    "REPORT_DIR = PROJECT_ROOT / \"reports\"\n",
    "JSONL_PATH = DATA_DIR / \"nlp_dataset.jsonl\"\n",
    "\n",
    "# Tạo thư mục nếu chưa tồn tại\n",
    "for d in [MODEL_DIR, REPORT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Thiết bị đang sử dụng: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02b2f6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang tải dữ liệu từ file JSONL...\n",
      "Đang xử lý tách từ và lọc stopwords...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xử lý văn bản: 100%|██████████| 115188/115188 [09:09<00:00, 209.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số lượng Train: 92150 | Số lượng Test: 23038\n",
      "Danh sách 20 nhãn: ['Bất động sản' 'Chứng khoán' 'Công nghệ' 'Du lịch' 'Gia đình'\n",
      " 'Giao thông' 'Giáo dục' 'Giải trí' 'Khoa học' 'Khởi nghiệp' 'Kinh doanh'\n",
      " 'Nông nghiệp' 'Pháp luật' 'Sức khỏe' 'Thế giới' 'Thể thao'\n",
      " 'Thời sự – Chính trị' 'Văn hóa' 'Đời sống' 'Ẩm thực']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. TẢI VÀ TIỀN XỬ LÝ DỮ LIỆU\n",
    "\n",
    "# Danh sách từ dừng (Stopwords)\n",
    "STOPWORDS = {\n",
    "    \"thì\", \"là\", \"mà\", \"của\", \"những\", \"các\", \"để\", \"và\", \"với\", \"có\", \n",
    "    \"trong\", \"đã\", \"đang\", \"sẽ\", \"được\", \"bị\", \"tại\", \"vì\", \"như\", \"này\",\n",
    "    \"cho\", \"về\", \"một\", \"người\", \"khi\", \"ra\", \"vào\", \"lên\", \"xuống\",\n",
    "    \"tôi\", \"chúng_tôi\", \"bạn\", \"họ\", \"chúng_ta\", \"theo\", \"ông\", \"bà\",\n",
    "    \"nhiều\", \"ít\", \"rất\", \"quá\", \"lắm\", \"nhưng\", \"tuy_nhiên\", \"nếu\", \"dù\",\n",
    "    \"bài\", \"viết\", \"ảnh\", \"video\", \"clip\", \"nguồn\"\n",
    "}\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Chuẩn hóa Unicode về dạng NFC.\"\"\"\n",
    "    return unicodedata.normalize('NFC', text)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Chuẩn hóa, tách từ và loại bỏ từ dừng.\"\"\"\n",
    "    text = normalize_text(text)\n",
    "    tokenized = ViTokenizer.tokenize(text)\n",
    "    words = tokenized.split()\n",
    "    clean_words = [w for w in words if w.lower() not in STOPWORDS]\n",
    "    return \" \".join(clean_words)\n",
    "\n",
    "if JSONL_PATH.exists():\n",
    "    print(\"Đang tải dữ liệu từ file JSONL...\")\n",
    "    df = pd.read_json(JSONL_PATH, lines=True)\n",
    "    \n",
    "    # Đảm bảo có cột raw_text (cho PhoBERT)\n",
    "    if 'raw_text' not in df.columns:\n",
    "        print(\"Đang tạo cột 'raw_text'...\")\n",
    "        df['raw_text'] = df['text'].apply(normalize_text)\n",
    "    \n",
    "    # Đảm bảo cột text đã được xử lý (cho ML/LSTM)\n",
    "    print(\"Đang xử lý tách từ và lọc stopwords...\")\n",
    "    tqdm.pandas(desc=\"Xử lý văn bản\")\n",
    "    df['text'] = df['raw_text'].progress_apply(preprocess_text)\n",
    "    \n",
    "    # Lưu lại file đã xử lý để lần sau dùng nhanh hơn\n",
    "    df.to_json(JSONL_PATH, orient=\"records\", lines=True)\n",
    "else:\n",
    "    print(\"Lỗi: Không tìm thấy file dữ liệu. Vui lòng kiểm tra lại thư mục data.\")\n",
    "\n",
    "# Mã hóa nhãn (Label Encoding)\n",
    "target_col = 'label_name'\n",
    "if target_col not in df.columns and 'label' in df.columns:\n",
    "    df[target_col] = df['label']\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['label_id'] = le.fit_transform(df[target_col])\n",
    "classes = le.classes_\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Chia tập Train/Test (Tỷ lệ 80/20)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[target_col])\n",
    "\n",
    "print(f\"Số lượng Train: {len(train_df)} | Số lượng Test: {len(test_df)}\")\n",
    "print(f\"Danh sách {num_classes} nhãn: {classes}\")\n",
    "\n",
    "del df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67524436",
   "metadata": {},
   "source": [
    "# 3. MÔ HÌNH MACHINE LEARNING CƠ BẢN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c560ad97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang tạo vector TF-IDF...\n",
      "Đang huấn luyện Naive Bayes...\n",
      "Naive Bayes Accuracy: 0.8235\n",
      "Đang huấn luyện Logistic Regression...\n",
      "Logistic Regression Accuracy: 0.8801\n",
      "Đang huấn luyện SVM (LinearSVC)...\n",
      "SVM (LinearSVC) Accuracy: 0.8863\n",
      "Đang huấn luyện SGD Classifier...\n",
      "SGD Classifier Accuracy: 0.8808\n"
     ]
    }
   ],
   "source": [
    "# Tạo đặc trưng TF-IDF\n",
    "print(\"Đang tạo vector TF-IDF...\")\n",
    "tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1, 2))\n",
    "X_train = tfidf.fit_transform(train_df['text'])\n",
    "X_test = tfidf.transform(test_df['text'])\n",
    "\n",
    "# 1. Naive Bayes\n",
    "print(\"Đang huấn luyện Naive Bayes...\")\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, train_df['label_id'])\n",
    "acc_nb = accuracy_score(test_df['label_id'], nb.predict(X_test))\n",
    "print(f\"Naive Bayes Accuracy: {acc_nb:.4f}\")\n",
    "\n",
    "# 2. Logistic Regression\n",
    "print(\"Đang huấn luyện Logistic Regression...\")\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
    "lr.fit(X_train, train_df['label_id'])\n",
    "acc_lr = accuracy_score(test_df['label_id'], lr.predict(X_test))\n",
    "print(f\"Logistic Regression Accuracy: {acc_lr:.4f}\")\n",
    "\n",
    "# 3. SVM (LinearSVC) - NÂNG CẤP CALIBRATION\n",
    "print(\"Đang huấn luyện SVM (LinearSVC) với chế độ chuẩn hóa xác suất...\")\n",
    "linear_svc = LinearSVC(dual=False, random_state=42, max_iter=1000)\n",
    "svm = CalibratedClassifierCV(linear_svc, method='sigmoid', cv=5) \n",
    "svm.fit(X_train, train_df['label_id'])\n",
    "acc_svm = accuracy_score(test_df['label_id'], svm.predict(X_test))\n",
    "print(f\"SVM (Calibrated) Accuracy: {acc_svm:.4f}\")\n",
    "\n",
    "# 4. SGD Classifier\n",
    "print(\"Đang huấn luyện SGD Classifier...\")\n",
    "sgd = SGDClassifier(loss='modified_huber', penalty='l2', alpha=1e-4, \n",
    "                    random_state=42, max_iter=1000, tol=1e-3, n_jobs=-1)\n",
    "sgd.fit(X_train, train_df['label_id'])\n",
    "acc_sgd = accuracy_score(test_df['label_id'], sgd.predict(X_test))\n",
    "print(f\"SGD Classifier Accuracy: {acc_sgd:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bc8470",
   "metadata": {},
   "source": [
    "# 4. DEEP LEARNING (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cb8e86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang huấn luyện mô hình LSTM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\envs\\trading\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1371: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:39.)\n",
      "  return t.to(\n",
      "Epoch 1: 100%|██████████| 1440/1440 [00:15<00:00, 92.54it/s]\n",
      "Epoch 2: 100%|██████████| 1440/1440 [00:15<00:00, 93.75it/s]\n",
      "Epoch 3: 100%|██████████| 1440/1440 [00:15<00:00, 93.80it/s]\n",
      "Epoch 4: 100%|██████████| 1440/1440 [00:15<00:00, 93.65it/s]\n",
      "Epoch 5: 100%|██████████| 1440/1440 [00:15<00:00, 93.59it/s]\n",
      "Epoch 6: 100%|██████████| 1440/1440 [00:15<00:00, 93.67it/s]\n",
      "Epoch 7: 100%|██████████| 1440/1440 [00:15<00:00, 93.75it/s]\n",
      "Epoch 8: 100%|██████████| 1440/1440 [00:15<00:00, 93.48it/s]\n",
      "Epoch 9: 100%|██████████| 1440/1440 [00:15<00:00, 93.71it/s]\n",
      "Epoch 10: 100%|██████████| 1440/1440 [00:15<00:00, 93.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM Accuracy: 0.8192\n"
     ]
    }
   ],
   "source": [
    "print(\"Đang huấn luyện mô hình LSTM...\")\n",
    "\n",
    "# Xây dựng bộ từ vựng (Vocabulary)\n",
    "counter = Counter()\n",
    "for t in train_df['text']: \n",
    "    counter.update(t.split())\n",
    "\n",
    "vocab = {w: i+2 for i, (w, _) in enumerate(counter.most_common(20000))}\n",
    "vocab['<PAD>'] = 0\n",
    "vocab['<UNK>'] = 1\n",
    "MAX_LEN_LSTM = 1024\n",
    "\n",
    "def text_to_seq(text, vocab, max_len):\n",
    "    seq = [vocab.get(w, 1) for w in text.split()]\n",
    "    if len(seq) < max_len:\n",
    "        seq += [0] * (max_len - len(seq))\n",
    "    return seq[:max_len]\n",
    "\n",
    "class LSTMDataset(Dataset):\n",
    "    def __init__(self, df): \n",
    "        self.x = [text_to_seq(t, vocab, MAX_LEN_LSTM) for t in df['text']]\n",
    "        self.y = df['label_id'].values\n",
    "    def __len__(self): \n",
    "        return len(self.y)\n",
    "    def __getitem__(self, idx): \n",
    "        return torch.tensor(self.x[idx]), torch.tensor(self.y[idx])\n",
    "\n",
    "train_loader = DataLoader(LSTMDataset(train_df), batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(LSTMDataset(test_df), batch_size=64)\n",
    "\n",
    "# Kiến trúc mô hình LSTM\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, (h_n, c_n) = self.lstm(embedded)\n",
    "        last_hidden = h_n[-1]\n",
    "        out = self.fc(last_hidden)\n",
    "        return out\n",
    "\n",
    "model_lstm = LSTMClassifier(len(vocab)+2, 128, 128, num_classes).to(DEVICE)\n",
    "optimizer = optim.Adam(model_lstm.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Vòng lặp huấn luyện\n",
    "for epoch in range(10):\n",
    "    model_lstm.train()\n",
    "    for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model_lstm(x), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Đánh giá\n",
    "model_lstm.eval()\n",
    "preds_lstm = []\n",
    "with torch.no_grad():\n",
    "    for x, _ in test_loader:\n",
    "        preds_lstm.extend(torch.argmax(model_lstm(x.to(DEVICE)), dim=1).cpu().numpy())\n",
    "\n",
    "acc_lstm = accuracy_score(test_df['label_id'], preds_lstm)\n",
    "print(f\"LSTM Accuracy: {acc_lstm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ed01d2",
   "metadata": {},
   "source": [
    "# 5. TRANSFORMERS (PHOBERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b62d8546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Đang chuẩn bị huấn luyện PhoBERT...\")\n",
    "\n",
    "# # Tham số cấu hình\n",
    "# MAX_LEN_BERT = 256\n",
    "# BATCH_SIZE = 32\n",
    "# LEARNING_RATE = 2e-5\n",
    "# EPOCHS = 5\n",
    "# MODEL_NAME = \"vinai/phobert-base-v2\"\n",
    "\n",
    "# PHOBERT_DIR = MODEL_DIR / \"phobert_best\"\n",
    "# PHOBERT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# class PhoBERTDataset(Dataset):\n",
    "#     def __init__(self, df, tokenizer, max_len):\n",
    "#         self.texts = df['raw_text'].tolist()\n",
    "#         self.labels = df['label_id'].tolist()\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_len = max_len\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         text = str(self.texts[idx])\n",
    "#         tokens = self.tokenizer.encode(text, add_special_tokens=True)\n",
    "        \n",
    "#         # Chiến thuật cắt ghép Head + Tail để lấy được thông tin đầu và cuối bài viết\n",
    "#         if len(tokens) > self.max_len:\n",
    "#             head_len = 128\n",
    "#             tail_len = self.max_len - head_len\n",
    "#             input_ids = tokens[:head_len] + tokens[-tail_len:]\n",
    "#         else:\n",
    "#             padding_len = self.max_len - len(tokens)\n",
    "#             input_ids = tokens + [self.tokenizer.pad_token_id] * padding_len\n",
    "            \n",
    "#         input_ids = torch.tensor(input_ids)\n",
    "#         attention_mask = (input_ids != self.tokenizer.pad_token_id).long()\n",
    "        \n",
    "#         return {\n",
    "#             'input_ids': input_ids,\n",
    "#             'attention_mask': attention_mask,\n",
    "#             'labels': torch.tensor(self.labels[idx])\n",
    "#         }\n",
    "\n",
    "# train_loader_bert = DataLoader(PhoBERTDataset(train_df, tokenizer, MAX_LEN_BERT), batch_size=BATCH_SIZE, shuffle=True)\n",
    "# test_loader_bert = DataLoader(PhoBERTDataset(test_df, tokenizer, MAX_LEN_BERT), batch_size=BATCH_SIZE)\n",
    "\n",
    "# model_bert = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_classes).to(DEVICE)\n",
    "# optimizer = AdamW(model_bert.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# best_acc = 0.0\n",
    "\n",
    "# for epoch in range(EPOCHS):\n",
    "#     print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "#     model_bert.train()\n",
    "#     train_bar = tqdm(train_loader_bert, desc=\"Training\")\n",
    "    \n",
    "#     for batch in train_bar:\n",
    "#         input_ids = batch['input_ids'].to(DEVICE)\n",
    "#         attention_mask = batch['attention_mask'].to(DEVICE)\n",
    "#         labels = batch['labels'].to(DEVICE)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model_bert(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "#         loss = outputs.loss\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         train_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "#     # Đánh giá\n",
    "#     model_bert.eval()\n",
    "#     preds, targets = [], []\n",
    "#     with torch.no_grad():\n",
    "#         for batch in tqdm(test_loader_bert, desc=\"Evaluating\"):\n",
    "#             outputs = model_bert(batch['input_ids'].to(DEVICE), attention_mask=batch['attention_mask'].to(DEVICE))\n",
    "#             preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "#             targets.extend(batch['labels'].numpy())\n",
    "    \n",
    "#     acc = accuracy_score(targets, preds)\n",
    "#     print(f\"Validation Accuracy: {acc:.4f}\")\n",
    "    \n",
    "#     if acc > best_acc:\n",
    "#         print(f\"Kỷ lục mới (Acc: {acc:.4f}). Đang lưu model...\")\n",
    "#         model_bert.save_pretrained(PHOBERT_DIR)\n",
    "#         tokenizer.save_pretrained(PHOBERT_DIR)\n",
    "#         best_acc = acc\n",
    "\n",
    "# print(f\"Độ chính xác tốt nhất của PhoBERT: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b0bfb1",
   "metadata": {},
   "source": [
    "# 6. ĐÁNH GIÁ & BÁO CÁO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12b924cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Đang lưu các mô hình...\n",
      "Đã lưu thành công tất cả model.\n",
      "\n",
      "Bảng Xếp Hạng Hiệu Năng:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Model",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Accuracy",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "bf249a6e-5c84-4ba7-8b07-116e18325fa6",
       "rows": [
        [
         "0",
         "SVM (LinearSVC)",
         "0.8862748502474173"
        ],
        [
         "1",
         "SGD Classifier",
         "0.8808056254883236"
        ],
        [
         "2",
         "Logistic Regression",
         "0.8801111207570101"
        ],
        [
         "3",
         "Naive Bayes",
         "0.8235089851549614"
        ],
        [
         "4",
         "LSTM",
         "0.8192117371299592"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM (LinearSVC)</td>\n",
       "      <td>0.886275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SGD Classifier</td>\n",
       "      <td>0.880806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.880111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>0.823509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LSTM</td>\n",
       "      <td>0.819212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy\n",
       "0      SVM (LinearSVC)  0.886275\n",
       "1       SGD Classifier  0.880806\n",
       "2  Logistic Regression  0.880111\n",
       "3          Naive Bayes  0.823509\n",
       "4                 LSTM  0.819212"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Lưu trữ tất cả model\n",
    "print(\"Đang lưu các mô hình...\")\n",
    "joblib.dump(le, MODEL_DIR / \"label_encoder.pkl\")\n",
    "joblib.dump(tfidf, MODEL_DIR / \"tfidf_vectorizer.pkl\")\n",
    "joblib.dump(nb, MODEL_DIR / \"naive_bayes.pkl\")\n",
    "joblib.dump(lr, MODEL_DIR / \"logistic_regression.pkl\")\n",
    "joblib.dump(svm, MODEL_DIR / \"svm_linear.pkl\")\n",
    "joblib.dump(sgd, MODEL_DIR / \"sgd_classifier.pkl\")\n",
    "\n",
    "lstm_checkpoint = {\n",
    "    'vocab': vocab, \n",
    "    'model_state': model_lstm.state_dict(),\n",
    "    'config': {\n",
    "        'vocab_size': len(vocab)+2, \n",
    "        'embed_dim': 100, \n",
    "        'hidden_dim': 100, \n",
    "        'num_classes': num_classes, \n",
    "        'max_len': MAX_LEN_LSTM\n",
    "    }\n",
    "}\n",
    "torch.save(lstm_checkpoint, MODEL_DIR / \"lstm_model.pth\")\n",
    "print(\"Đã lưu thành công tất cả model.\")\n",
    "\n",
    "# 2. Tạo bảng tổng hợp kết quả\n",
    "results_df = pd.DataFrame([\n",
    "    {\"Model\": \"SVM (LinearSVC)\", \"Accuracy\": acc_svm},\n",
    "    {\"Model\": \"SGD Classifier\", \"Accuracy\": acc_sgd},\n",
    "    {\"Model\": \"Logistic Regression\", \"Accuracy\": acc_lr},\n",
    "    {\"Model\": \"Naive Bayes\", \"Accuracy\": acc_nb},\n",
    "    {\"Model\": \"LSTM\", \"Accuracy\": acc_lstm},\n",
    "    # {\"Model\": \"PhoBERT (Best)\", \"Accuracy\": best_acc}\n",
    "]).sort_values(by=\"Accuracy\", ascending=False)\n",
    "\n",
    "print(\"\\nBảng Xếp Hạng Hiệu Năng:\")\n",
    "display(results_df)\n",
    "results_df.to_excel(REPORT_DIR / \"final_leaderboard.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abc1f78",
   "metadata": {},
   "source": [
    "# 7. HỆ THỐNG DỰ ĐOÁN THỰC TẾ (INFERENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a476734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n",
      "Input: https://vnexpress.net/new-york-cai-cach-nha-tu-sau-vu-quan-g...\n",
      "-----------------------------------------------------------------\n",
      "MODEL                | LABEL                          | CONF\n",
      "-----------------------------------------------------------------\n",
      "Naive Bayes          | THẾ GIỚI                       | 99.38%\n",
      "Logistic Reg         | THẾ GIỚI                       | 81.42%\n",
      "SVM                  | THẾ GIỚI                       | 27.90%\n",
      "SGD                  | THẾ GIỚI                       | 88.51%\n",
      "LSTM                 | THẾ GIỚI                       | 98.17%\n",
      "PhoBERT              | THẾ GIỚI                       | 99.40%\n",
      "-----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# 7. HỆ THỐNG DỰ ĐOÁN (FINAL POLISH - FULL PROBABILITY)\n",
    "# ==============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import joblib\n",
    "import requests\n",
    "import numpy as np # Cần numpy để tính Softmax cho SVM\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from pyvi import ViTokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# --- CẤU HÌNH ---\n",
    "CURRENT_DIR = Path.cwd()\n",
    "PROJECT_ROOT = CURRENT_DIR if (CURRENT_DIR / \"data\").exists() else CURRENT_DIR.parent\n",
    "MODEL_DIR = PROJECT_ROOT / \"models\"\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "# --- LSTM CLASS ---\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, (h_n, c_n) = self.lstm(embedded)\n",
    "        return self.fc(h_n[-1])\n",
    "\n",
    "# --- LOAD MODELS ---\n",
    "le, tfidf = None, None\n",
    "nb, lr, svm, sgd = None, None, None, None\n",
    "lstm_model, lstm_vocab, lstm_config = None, {}, {}\n",
    "model_bert, tokenizer_bert = None, None\n",
    "\n",
    "try:\n",
    "    # 1. Sklearn\n",
    "    if (MODEL_DIR / \"label_encoder.pkl\").exists(): le = joblib.load(MODEL_DIR / \"label_encoder.pkl\")\n",
    "    if (MODEL_DIR / \"tfidf_vectorizer.pkl\").exists(): tfidf = joblib.load(MODEL_DIR / \"tfidf_vectorizer.pkl\")\n",
    "    if (MODEL_DIR / \"naive_bayes.pkl\").exists(): nb = joblib.load(MODEL_DIR / \"naive_bayes.pkl\")\n",
    "    if (MODEL_DIR / \"logistic_regression.pkl\").exists(): lr = joblib.load(MODEL_DIR / \"logistic_regression.pkl\")\n",
    "    if (MODEL_DIR / \"svm_linear.pkl\").exists(): svm = joblib.load(MODEL_DIR / \"svm_linear.pkl\")\n",
    "    if (MODEL_DIR / \"sgd_classifier.pkl\").exists(): sgd = joblib.load(MODEL_DIR / \"sgd_classifier.pkl\")\n",
    "\n",
    "    # 2. LSTM (FIX SIZE 128)\n",
    "    if (MODEL_DIR / \"lstm_model.pth\").exists():\n",
    "        ckpt = torch.load(MODEL_DIR / \"lstm_model.pth\", map_location='cpu')\n",
    "        lstm_config, lstm_vocab = ckpt['config'], ckpt['vocab']\n",
    "        \n",
    "        # Hardcode 128 để khớp với file đã train\n",
    "        lstm_model = LSTMClassifier(\n",
    "            lstm_config.get('vocab_size', len(lstm_vocab) + 2),\n",
    "            128, 128, \n",
    "            lstm_config.get('num_classes', 20)\n",
    "        )\n",
    "        lstm_model.load_state_dict(ckpt['model_state'])\n",
    "        lstm_model.to(DEVICE).eval()\n",
    "\n",
    "    # 3. PhoBERT\n",
    "    PHOBERT_PATH = MODEL_DIR / \"phobert_best\"\n",
    "    if PHOBERT_PATH.exists():\n",
    "        tokenizer_bert = AutoTokenizer.from_pretrained(PHOBERT_PATH)\n",
    "        model_bert = AutoModelForSequenceClassification.from_pretrained(PHOBERT_PATH).to(DEVICE)\n",
    "        model_bert.eval()\n",
    "    elif 'model_bert' in globals() and model_bert:\n",
    "        tokenizer_bert = globals().get('tokenizer')\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Load Error: {e}\")\n",
    "\n",
    "# --- PREDICT FUNCTION ---\n",
    "def predict_all_models(url_or_text):\n",
    "    print(f\"\\nInput: {url_or_text[:60]}...\")\n",
    "    \n",
    "    # Get Content\n",
    "    content = url_or_text\n",
    "    if url_or_text.startswith(\"http\"):\n",
    "        try:\n",
    "            headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "            resp = requests.get(url_or_text, headers=headers, timeout=10)\n",
    "            soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "            content = ' '.join([p.get_text() for p in soup.find_all('p')])\n",
    "            if len(content) < 50: return print(\"Short content.\")\n",
    "        except: return print(\"URL Error.\")\n",
    "\n",
    "    # Preprocess\n",
    "    text_seg = ViTokenizer.tokenize(content)\n",
    "    \n",
    "    print(\"-\" * 65)\n",
    "    print(f\"{'MODEL':<20} | {'LABEL':<30} | {'CONF'}\")\n",
    "    print(\"-\" * 65)\n",
    "\n",
    "    # 1. ML Predict\n",
    "    if tfidf and le:\n",
    "        vec = tfidf.transform([text_seg])\n",
    "        \n",
    "        # Naive Bayes\n",
    "        if nb:\n",
    "            print(f\"{'Naive Bayes':<20} | {le.inverse_transform(nb.predict(vec))[0].upper():<30} | {nb.predict_proba(vec).max():.2%}\")\n",
    "        \n",
    "        # Logistic Regression\n",
    "        if lr:\n",
    "            print(f\"{'Logistic Reg':<20} | {le.inverse_transform(lr.predict(vec))[0].upper():<30} | {lr.predict_proba(vec).max():.2%}\")\n",
    "        \n",
    "        # SVM (LinearSVC)\n",
    "        if svm:\n",
    "            pred = svm.predict(vec)[0]\n",
    "            l = le.inverse_transform([pred])[0]\n",
    "            try:\n",
    "                dec = svm.decision_function(vec) \n",
    "                probs = np.exp(dec) / np.sum(np.exp(dec), axis=1, keepdims=True)\n",
    "                p = f\"{probs.max():.2%}\"\n",
    "            except:\n",
    "                p = \"N/A\"\n",
    "            print(f\"{'SVM':<20} | {l.upper():<30} | {p}\")\n",
    "\n",
    "        # SGD\n",
    "        if sgd:\n",
    "            p = f\"{sgd.predict_proba(vec).max():.2%}\" if hasattr(sgd, \"predict_proba\") else \"N/A\"\n",
    "            print(f\"{'SGD':<20} | {le.inverse_transform(sgd.predict(vec))[0].upper():<30} | {p}\")\n",
    "\n",
    "    # 2. LSTM Predict\n",
    "    if lstm_model and lstm_vocab:\n",
    "        max_len = lstm_config.get('max_len', 1024)\n",
    "        seq = [lstm_vocab.get(w, 1) for w in text_seg.split()]\n",
    "        seq = (seq + [0]*(max_len-len(seq)))[:max_len]\n",
    "        \n",
    "        model_device = next(lstm_model.parameters()).device\n",
    "        seq_tensor = torch.tensor([seq], dtype=torch.long).to(model_device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = lstm_model(seq_tensor)\n",
    "            prob, idx = torch.max(torch.softmax(out, dim=1), dim=1)\n",
    "            print(f\"{'LSTM':<20} | {le.inverse_transform([idx.item()])[0].upper():<30} | {prob.item():.2%}\")\n",
    "\n",
    "    # 3. PhoBERT Predict\n",
    "    if model_bert and tokenizer_bert:\n",
    "        tokens = tokenizer_bert.encode(content, add_special_tokens=True)\n",
    "        ids = (tokens[:128] + tokens[-128:]) if len(tokens) > 256 else (tokens + [tokenizer_bert.pad_token_id]*(256-len(tokens)))\n",
    "        \n",
    "        model_device = next(model_bert.parameters()).device\n",
    "        ids_t = torch.tensor([ids]).to(model_device)\n",
    "        mask_t = (ids_t != tokenizer_bert.pad_token_id).long().to(model_device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            out = model_bert(ids_t, attention_mask=mask_t)\n",
    "            prob, idx = torch.max(torch.softmax(out.logits, dim=1), dim=1)\n",
    "            print(f\"{'PhoBERT':<20} | {le.inverse_transform([idx.item()])[0].upper():<30} | {prob.item():.2%}\")\n",
    "    \n",
    "    print(\"-\" * 65)\n",
    "\n",
    "# --- TEST ---\n",
    "link_test = \"https://vnexpress.net/new-york-cai-cach-nha-tu-sau-vu-quan-giao-danh-chet-pham-nhan-4996107.html\"\n",
    "predict_all_models(link_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (trading)",
   "language": "python",
   "name": "trading"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
