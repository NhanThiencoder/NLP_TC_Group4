{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quy trình Tiền xử lý Dữ liệu (Data Preprocessing Pipeline)\n",
    "\n",
    "Notebook này thực hiện quy trình làm sạch dữ liệu thô từ các file văn bản (.txt) được tổ chức theo thư mục chủ đề.\n",
    "\n",
    "**Các bước xử lý chính:**\n",
    "1.  **Làm sạch nội dung (Cleaning):** Quét toàn bộ file, loại bỏ các dòng chứa đường dẫn (URL) hoặc metadata rác.\n",
    "2.  **Tải và Chuẩn hóa:** Đọc dữ liệu, xử lý các lỗi mã hóa (Encoding) phổ biến (UTF-8, UTF-16).\n",
    "3.  **Khử trùng lặp (Deduplication):** Loại bỏ các bài viết có nội dung giống nhau hoàn toàn.\n",
    "4.  **Lọc bài viết ngắn (Short Content Filter):** Loại bỏ các bài viết không đủ độ dài tối thiểu (ví dụ: < 100 ký tự).\n",
    "5.  **Lọc nhiễu bằng ML (Label Cleaning):** Sử dụng mô hình Logistic Regression để phát hiện và loại bỏ các bài viết bị đặt sai thư mục chủ đề.\n",
    "6.  **Xuất dữ liệu:** Lưu bộ dữ liệu sạch ra file `nlp_dataset.jsonl` để sử dụng cho huấn luyện."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. THIẾT LẬP MÔI TRƯỜNG\n",
    "import os\n",
    "import shutil\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from pyvi import ViTokenizer\n",
    "\n",
    "# Cấu hình đường dẫn\n",
    "CURRENT_DIR = Path.cwd()\n",
    "PROJECT_ROOT = CURRENT_DIR if (CURRENT_DIR / \"data\").exists() else CURRENT_DIR.parent\n",
    "\n",
    "RAW_DATA_DIR = PROJECT_ROOT / \"data\" / \"raw\"      # Nơi chứa các folder .txt gốc\n",
    "FINAL_DATA_DIR = PROJECT_ROOT / \"data\" / \"final\"  # Nơi lưu jsonl sạch\n",
    "DISCARD_DIR = PROJECT_ROOT / \"data\" / \"discarded\" # Nơi chứa các file bị loại bỏ\n",
    "\n",
    "# Tạo các thư mục cần thiết\n",
    "for d in [FINAL_DATA_DIR, DISCARD_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "print(f\"Thư mục dữ liệu gốc: {RAW_DATA_DIR}\")\n",
    "print(f\"Thư mục lưu kết quả: {FINAL_DATA_DIR}\")\n",
    "print(f\"Thư mục chứa file loại bỏ: {DISCARD_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. LÀM SẠCH NỘI DUNG (URL REMOVAL)\n",
    "Bước này quét qua các file `.txt`, loại bỏ các dòng chứa liên kết web (http/https) vì chúng không đóng góp vào việc phân loại văn bản."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_file_content(file_path):\n",
    "    \"\"\"Đọc file, loại bỏ dòng chứa URL và ghi lại file.\"\"\"\n",
    "    try:\n",
    "        # Thử đọc với UTF-16 trước (do dữ liệu gốc hay dùng format này)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-16') as f:\n",
    "                lines = f.readlines()\n",
    "        except UnicodeError:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                lines = f.readlines()\n",
    "        \n",
    "        # Lọc bỏ dòng chứa URL\n",
    "        clean_lines = [line for line in lines if \"http\" not in line]\n",
    "        \n",
    "        # Ghi đè lại file (luôn lưu UTF-8 cho thống nhất)\n",
    "        if len(clean_lines) != len(lines):\n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                f.writelines(clean_lines)\n",
    "            return True # Có thay đổi\n",
    "    except Exception as e:\n",
    "        print(f\"Lỗi xử lý file {file_path.name}: {e}\")\n",
    "    return False\n",
    "\n",
    "print(\"Đang quét và làm sạch URL trong các file văn bản...\")\n",
    "files = list(RAW_DATA_DIR.glob(\"**/*.txt\"))\n",
    "changed_count = 0\n",
    "\n",
    "for file_path in tqdm(files, desc=\"URL Cleaning\"):\n",
    "    if clean_file_content(file_path):\n",
    "        changed_count += 1\n",
    "\n",
    "print(f\"Đã xử lý xong. Số file được làm sạch: {changed_count}/{len(files)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. TẢI DỮ LIỆU VÀO DATAFRAME\n",
    "Đọc toàn bộ file `.txt` vào Pandas DataFrame để dễ dàng thao tác lọc và xử lý."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_to_df(data_dir):\n",
    "    data = []\n",
    "    files = list(data_dir.glob(\"**/*.txt\"))\n",
    "    \n",
    "    print(f\"Đang tải {len(files)} file vào bộ nhớ...\")\n",
    "    for file_path in tqdm(files, desc=\"Loading Data\"):\n",
    "        content = \"\"\n",
    "        try:\n",
    "            # Ưu tiên đọc UTF-16, fallback sang UTF-8\n",
    "            try:\n",
    "                with open(file_path, \"r\", encoding=\"utf-16\") as f: \n",
    "                    content = f.read().strip()\n",
    "            except:\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f: \n",
    "                    content = f.read().strip()\n",
    "            \n",
    "            if content:\n",
    "                # Chuẩn hóa Unicode ngay lập tức\n",
    "                content = unicodedata.normalize('NFC', content)\n",
    "                data.append({\n",
    "                    \"raw_text\": content,\n",
    "                    \"label_name\": file_path.parent.name,\n",
    "                    \"filename\": file_path.name,\n",
    "                    \"file_path\": str(file_path)\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Không thể đọc file {file_path}: {e}\")\n",
    "            continue\n",
    "            \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df = load_data_to_df(RAW_DATA_DIR)\n",
    "print(f\"Đã tải thành công: {len(df)} dòng dữ liệu.\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. LỌC TRÙNG LẶP VÀ BÀI VIẾT QUÁ NGẮN\n",
    "- **Deduplication:** Sử dụng hàm băm (MD5) để phát hiện bài trùng.\n",
    "- **Short Text Filter:** Loại bỏ bài viết dưới 70 ký tự (thường là lỗi crawl hoặc tiêu đề cụt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_discarded_files(df_discard, reason_folder):\n",
    "    \"\"\"Di chuyển các file bị loại bỏ vào thư mục riêng để kiểm tra lại nếu cần.\"\"\"\n",
    "    target_dir = DISCARD_DIR / reason_folder\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for _, row in df_discard.iterrows():\n",
    "        src = Path(row['file_path'])\n",
    "        dst = target_dir / f\"{row['label_name']}_{row['filename']}\"\n",
    "        try:\n",
    "            if src.exists():\n",
    "                shutil.move(str(src), str(dst))\n",
    "        except Exception as e:\n",
    "            print(f\"Lỗi di chuyển file {src.name}: {e}\")\n",
    "\n",
    "# 1. Xử lý trùng lặp\n",
    "df['content_hash'] = df['raw_text'].apply(lambda x: hashlib.md5(x.encode()).hexdigest())\n",
    "duplicates = df[df.duplicated(subset='content_hash', keep='first')]\n",
    "df_clean = df.drop_duplicates(subset='content_hash', keep='first')\n",
    "\n",
    "print(f\"Phát hiện {len(duplicates)} bài viết trùng lặp. Đang di chuyển...\")\n",
    "move_discarded_files(duplicates, \"duplicates\")\n",
    "\n",
    "# 2. Xử lý bài viết ngắn\n",
    "MIN_LENGTH = 100\n",
    "short_articles = df_clean[df_clean['raw_text'].str.len() < MIN_LENGTH]\n",
    "df_clean = df_clean[df_clean['raw_text'].str.len() >= MIN_LENGTH]\n",
    "\n",
    "print(f\"Phát hiện {len(short_articles)} bài viết quá ngắn (<{MIN_LENGTH} ký tự). Đang di chuyển...\")\n",
    "move_discarded_files(short_articles, \"too_short\")\n",
    "\n",
    "print(f\"\\nDữ liệu còn lại sau lọc sơ bộ: {len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. LỌC NHIỄU NHÃN BẰNG MACHINE LEARNING (LABEL CLEANING)\n",
    "Sử dụng mô hình Logistic Regression để tìm các bài viết nằm sai thư mục (Outliers/Mislabeling).\n",
    "Ví dụ: Bài viết về \"Thể thao\" nhưng lại nằm trong thư mục \"Kinh doanh\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Đang chuẩn bị dữ liệu để lọc nhiễu nhãn...\")\n",
    "\n",
    "# Tiền xử lý sơ bộ (Tách từ) cho mô hình lọc nhiễu\n",
    "tqdm.pandas(desc=\"Tokenizing for Cleaning\")\n",
    "df_clean['text_tokenized'] = df_clean['raw_text'].progress_apply(ViTokenizer.tokenize)\n",
    "\n",
    "# Vector hóa TF-IDF\n",
    "tfidf_cleaner = TfidfVectorizer(max_features=10000)\n",
    "X = tfidf_cleaner.fit_transform(df_clean['text_tokenized'])\n",
    "\n",
    "# Mã hóa nhãn\n",
    "le_cleaner = LabelEncoder()\n",
    "y = le_cleaner.fit_transform(df_clean['label_name'])\n",
    "\n",
    "# Huấn luyện Logistic Regression và dự đoán (Cross Validation để tránh Overfitting)\n",
    "print(\"Đang chạy Cross-Validation để phát hiện nhãn sai (có thể mất vài phút)...\")\n",
    "clf = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "\n",
    "# Lấy xác suất dự đoán cho từng mẫu\n",
    "y_probs = cross_val_predict(clf, X, y, cv=3, method='predict_proba')\n",
    "y_pred = y_probs.argmax(axis=1)\n",
    "y_conf = y_probs.max(axis=1)\n",
    "\n",
    "# Xác định các mẫu nhiễu\n",
    "# Tiêu chí: Mô hình dự đoán khác nhãn gốc VÀ độ tin cậy > 0.85 (Rất chắc chắn là sai)\n",
    "CONFIDENCE_THRESHOLD = 0.85\n",
    "mask_wrong = (y_pred != y) & (y_conf > CONFIDENCE_THRESHOLD)\n",
    "wrong_indices = df_clean.index[mask_wrong]\n",
    "\n",
    "df_wrong = df_clean.loc[wrong_indices].copy()\n",
    "df_wrong['predicted_label'] = le_cleaner.inverse_transform(y_pred[mask_wrong])\n",
    "\n",
    "print(f\"\\nPhát hiện {len(df_wrong)} bài viết có khả năng bị gán nhãn sai (Độ tin cậy > {CONFIDENCE_THRESHOLD}).\")\n",
    "if len(df_wrong) > 0:\n",
    "    print(\"Ví dụ 3 mẫu sai:\")\n",
    "    display(df_wrong[['label_name', 'predicted_label', 'raw_text']].head(3))\n",
    "\n",
    "# Loại bỏ các mẫu sai khỏi tập dữ liệu chính\n",
    "df_final = df_clean.drop(index=wrong_indices)\n",
    "move_discarded_files(df_wrong, \"wrong_label_detected\")\n",
    "\n",
    "print(f\"\\nKích thước dữ liệu cuối cùng: {len(df_final)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. XUẤT DỮ LIỆU SẠCH (FINAL EXPORT)\n",
    "Lưu dữ liệu đã qua xử lý ra file `nlp_dataset.jsonl`. Đây sẽ là đầu vào cho file `EDA.ipynb` và `model.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo cột 'text' chính thức (đã tách từ) để dùng cho các bước sau\n",
    "df_final['text'] = df_final['text_tokenized']\n",
    "\n",
    "# Chỉ giữ lại các cột cần thiết\n",
    "cols_to_save = ['text', 'raw_text', 'label_name', 'filename']\n",
    "df_export = df_final[cols_to_save]\n",
    "\n",
    "print(f\"Đang lưu dữ liệu ra: {JSONL_PATH}\")\n",
    "df_export.to_json(JSONL_PATH, orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "print(\"✅ Quy trình tiền xử lý hoàn tất!\")\n",
    "print(\"Sẵn sàng chuyển sang bước EDA và Modeling.\")\n",
    "\n",
    "# Thống kê cuối cùng theo chủ đề\n",
    "print(\"\\nThống kê số lượng bài viết theo chủ đề:\")\n",
    "print(df_export['label_name'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}