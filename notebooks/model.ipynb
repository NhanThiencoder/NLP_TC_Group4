{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† NLP Project: Final Optimized Version (V5)\n",
    "\n",
    "Phi√™n b·∫£n t·ªëi ∆∞u h√≥a to√†n di·ªán:\n",
    "1.  **Machine Learning**: NB, LR, SVM (Baseline).\n",
    "2.  **Deep Learning**: LSTM (Fixed architecture).\n",
    "3.  **PhoBERT**: Optimized (Max Len 512, Save Best Model).\n",
    "4.  **Analysis**: Confusion Matrix & Error Inspection.\n",
    "5.  **Deployment**: Auto-save models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# --- 1. SETUP & IMPORTS ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "import shutil\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "\n",
    "# SKLEARN\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# DEEP LEARNING\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from pyvi import ViTokenizer\n",
    "\n",
    "# --- C·∫§U H√åNH ---\n",
    "CURRENT_DIR = Path.cwd()\n",
    "if (CURRENT_DIR / \"data\").exists():\n",
    "    PROJECT_ROOT = CURRENT_DIR\n",
    "elif (CURRENT_DIR.parent / \"data\").exists():\n",
    "    PROJECT_ROOT = CURRENT_DIR.parent\n",
    "else:\n",
    "    PROJECT_ROOT = CURRENT_DIR\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"final\"\n",
    "JSONL_PATH = DATA_DIR / \"nlp_dataset.jsonl\"\n",
    "MODEL_DIR = PROJECT_ROOT / \"models\"\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORT_DIR = PROJECT_ROOT / \"reports\"\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úÖ Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ ƒêang t·∫£i d·ªØ li·ªáu...\n",
      "üìä Train: 92150 | Test: 23038 | Classes: 20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- 2. LOAD DATA (AUTO FIX & CLEAN) ---\n",
    "STOPWORDS = set([\n",
    "    \"th√¨\", \"l√†\", \"m√†\", \"c·ªßa\", \"nh·ªØng\", \"c√°c\", \"ƒë·ªÉ\", \"v√†\", \"v·ªõi\", \"c√≥\", \n",
    "    \"trong\", \"ƒë√£\", \"ƒëang\", \"s·∫Ω\", \"ƒë∆∞·ª£c\", \"b·ªã\", \"t·∫°i\", \"v√¨\", \"nh∆∞\", \"n√†y\",\n",
    "    \"cho\", \"v·ªÅ\", \"m·ªôt\", \"ng∆∞·ªùi\", \"khi\", \"ra\", \"v√†o\", \"l√™n\", \"xu·ªëng\",\n",
    "    \"t√¥i\", \"ch√∫ng_t√¥i\", \"b·∫°n\", \"h·ªç\", \"ch√∫ng_ta\", \"theo\", \"√¥ng\", \"b√†\",\n",
    "    \"nhi·ªÅu\", \"√≠t\", \"r·∫•t\", \"qu√°\", \"l·∫Øm\", \"nh∆∞ng\", \"tuy_nhi√™n\", \"n·∫øu\", \"d√π\",\n",
    "    \"b√†i\", \"vi·∫øt\", \"·∫£nh\", \"video\", \"clip\", \"ngu·ªìn\"\n",
    "])\n",
    "\n",
    "def normalize_text(text):\n",
    "    return unicodedata.normalize('NFC', text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    return \" \".join([w for w in words if w.lower() not in STOPWORDS])\n",
    "\n",
    "print(\"‚è≥ ƒêang t·∫£i d·ªØ li·ªáu...\")\n",
    "rebuild = False\n",
    "required_cols = {'text', 'raw_text', 'label_name'}\n",
    "\n",
    "if JSONL_PATH.exists():\n",
    "    try:\n",
    "        df = pd.read_json(JSONL_PATH, lines=True)\n",
    "        if not required_cols.issubset(df.columns):\n",
    "            print(\"‚ö†Ô∏è File c≈© thi·∫øu c·ªôt -> T√°i t·∫°o...\")\n",
    "            rebuild = True\n",
    "    except: rebuild = True\n",
    "else:\n",
    "    rebuild = True\n",
    "\n",
    "if rebuild and DATA_DIR.exists():\n",
    "    print(\"‚ôªÔ∏è ƒêang qu√©t d·ªØ li·ªáu g·ªëc...\")\n",
    "    data = []\n",
    "    files = list(DATA_DIR.glob(\"**/*.txt\"))\n",
    "    # files = files[:2000] # Uncomment ƒë·ªÉ test nhanh\n",
    "    for file_path in tqdm(files, desc=\"Processing\"):\n",
    "        try:\n",
    "            try: \n",
    "                with open(file_path, \"r\", encoding=\"utf-16\") as f: content = f.read().strip()\n",
    "            except: \n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f: content = f.read().strip()\n",
    "            \n",
    "            if content:\n",
    "                content = normalize_text(content)\n",
    "                tokenized = ViTokenizer.tokenize(content)\n",
    "                clean = remove_stopwords(tokenized)\n",
    "                data.append({\n",
    "                    \"text\": clean, \"raw_text\": content,\n",
    "                    \"label_name\": file_path.parent.name, \"filename\": file_path.name\n",
    "                })\n",
    "        except: continue\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_json(JSONL_PATH, orient=\"records\", lines=True)\n",
    "\n",
    "# Encode Nh√£n\n",
    "le = LabelEncoder()\n",
    "df['label_id'] = le.fit_transform(df['label_name'])\n",
    "classes = le.classes_\n",
    "num_classes = len(classes)\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['label_name'])\n",
    "print(f\"üìä Train: {len(train_df)} | Test: {len(test_df)} | Classes: {num_classes}\")\n",
    "del df; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è 1. Machine Learning Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ T·∫°o TF-IDF...\n",
      "‚öîÔ∏è Training Naive Bayes...\n",
      "‚úÖ NB Accuracy: 0.8235\n",
      "‚öîÔ∏è Training Logistic Regression...\n",
      "‚úÖ LR Accuracy: 0.8801\n",
      "‚öîÔ∏è Training SVM...\n",
      "‚úÖ SVM Accuracy: 0.8863\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "       B·∫•t ƒë·ªông s·∫£n       0.89      0.91      0.90      1114\n",
      "        Ch·ª©ng kho√°n       0.93      0.92      0.92       734\n",
      "          C√¥ng ngh·ªá       0.94      0.97      0.96      1369\n",
      "            Du l·ªãch       0.87      0.88      0.88      1056\n",
      "           Gia ƒë√¨nh       0.88      0.83      0.86       604\n",
      "         Giao th√¥ng       0.88      0.86      0.87       580\n",
      "           Gi√°o d·ª•c       0.83      0.85      0.84       634\n",
      "           Gi·∫£i tr√≠       0.81      0.77      0.79       809\n",
      "           Khoa h·ªçc       0.85      0.86      0.86       932\n",
      "        Kh·ªüi nghi·ªáp       0.81      0.65      0.72       760\n",
      "         Kinh doanh       0.87      0.87      0.87      1646\n",
      "        N√¥ng nghi·ªáp       0.82      0.76      0.79       704\n",
      "          Ph√°p lu·∫≠t       0.93      0.95      0.94      1506\n",
      "           S·ª©c kh·ªèe       0.94      0.96      0.95      1893\n",
      "           Th·∫ø gi·ªõi       0.93      0.95      0.94      1865\n",
      "           Th·ªÉ thao       0.98      0.99      0.99      2291\n",
      "Th·ªùi s·ª± ‚Äì Ch√≠nh tr·ªã       0.79      0.82      0.81      1860\n",
      "            VƒÉn h√≥a       0.83      0.86      0.85      1067\n",
      "           ƒê·ªùi s·ªëng       0.76      0.74      0.75       899\n",
      "            ·∫®m th·ª±c       0.90      0.90      0.90       715\n",
      "\n",
      "           accuracy                           0.89     23038\n",
      "          macro avg       0.87      0.86      0.87     23038\n",
      "       weighted avg       0.89      0.89      0.89     23038\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"‚è≥ T·∫°o TF-IDF...\")\n",
    "tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1, 2))\n",
    "X_train = tfidf.fit_transform(train_df['text'])\n",
    "X_test = tfidf.transform(test_df['text'])\n",
    "\n",
    "# 1. NB\n",
    "print(\"‚öîÔ∏è Training Naive Bayes...\")\n",
    "nb = MultinomialNB()\n",
    "nb.fit(X_train, train_df['label_id'])\n",
    "acc_nb = accuracy_score(test_df['label_id'], nb.predict(X_test))\n",
    "print(f\"‚úÖ NB Accuracy: {acc_nb:.4f}\")\n",
    "\n",
    "# 2. LR\n",
    "print(\"‚öîÔ∏è Training Logistic Regression...\")\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train, train_df['label_id'])\n",
    "acc_lr = accuracy_score(test_df['label_id'], lr.predict(X_test))\n",
    "print(f\"‚úÖ LR Accuracy: {acc_lr:.4f}\")\n",
    "\n",
    "# 3. SVM\n",
    "print(\"‚öîÔ∏è Training SVM...\")\n",
    "svm = LinearSVC(dual=False, random_state=42)\n",
    "svm.fit(X_train, train_df['label_id'])\n",
    "y_pred_svm = svm.predict(X_test)\n",
    "acc_svm = accuracy_score(test_df['label_id'], y_pred_svm)\n",
    "print(f\"‚úÖ SVM Accuracy: {acc_svm:.4f}\")\n",
    "print(classification_report(test_df['label_id'], y_pred_svm, target_names=classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† 2. LSTM (Fixed Architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Training LSTM...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ASUS\\anaconda3\\envs\\trading\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1371: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:39.)\n",
      "  return t.to(\n",
      "Epoch 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1440/1440 [00:08<00:00, 179.02it/s]\n",
      "Epoch 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1440/1440 [00:07<00:00, 185.75it/s]\n",
      "Epoch 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1440/1440 [00:07<00:00, 185.27it/s]\n",
      "Epoch 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1440/1440 [00:07<00:00, 185.51it/s]\n",
      "Epoch 5: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1440/1440 [00:07<00:00, 185.16it/s]\n",
      "Epoch 6: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1440/1440 [00:07<00:00, 185.44it/s]\n",
      "Epoch 7: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1440/1440 [00:07<00:00, 185.14it/s]\n",
      "Epoch 8: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1440/1440 [00:07<00:00, 185.63it/s]\n",
      "Epoch 9: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1440/1440 [00:07<00:00, 185.21it/s]\n",
      "Epoch 10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1440/1440 [00:07<00:00, 185.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LSTM Accuracy: 0.8441\n"
     ]
    }
   ],
   "source": [
    "print(\"‚è≥ Training LSTM...\")\n",
    "counter = Counter()\n",
    "for t in train_df['text']: counter.update(t.split())\n",
    "vocab = {w: i+2 for i, (w, _) in enumerate(counter.most_common(20000))}\n",
    "vocab['<PAD>'] = 0; vocab['<UNK>'] = 1\n",
    "MAX_LEN_LSTM = 500\n",
    "\n",
    "def text_to_seq(text, vocab, max_len):\n",
    "    seq = [vocab.get(w, 1) for w in text.split()]\n",
    "    if len(seq) < max_len: seq += [0]*(max_len-len(seq))\n",
    "    return seq[:max_len]\n",
    "\n",
    "class LSTMDataset(Dataset):\n",
    "    def __init__(self, df): \n",
    "        self.x = [text_to_seq(t, vocab, MAX_LEN_LSTM) for t in df['text']]\n",
    "        self.y = df['label_id'].values\n",
    "    def __len__(self): return len(self.y)\n",
    "    def __getitem__(self, idx): return torch.tensor(self.x[idx]), torch.tensor(self.y[idx])\n",
    "\n",
    "train_loader = DataLoader(LSTMDataset(train_df), batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(LSTMDataset(test_df), batch_size=64)\n",
    "\n",
    "# --- CUSTOM CLASS ƒê·ªÇ FIX L·ªñI TUPLE ---\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        output, (h_n, c_n) = self.lstm(embedded)\n",
    "        last_hidden = h_n[-1] # L·∫•y hidden state cu·ªëi c√πng\n",
    "        out = self.fc(last_hidden)\n",
    "        return out\n",
    "\n",
    "model_lstm = LSTMClassifier(len(vocab)+2, 100, 100, num_classes).to(device)\n",
    "opt = optim.Adam(model_lstm.parameters(), lr=0.001)\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(10):\n",
    "    model_lstm.train()\n",
    "    for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        opt.zero_grad(); loss = crit(model_lstm(x), y); loss.backward(); opt.step()\n",
    "\n",
    "model_lstm.eval()\n",
    "preds_lstm = []\n",
    "with torch.no_grad():\n",
    "    for x, _ in test_loader:\n",
    "        preds_lstm.extend(torch.argmax(model_lstm(x.to(device)), dim=1).cpu().numpy())\n",
    "\n",
    "acc_lstm = accuracy_score(test_df['label_id'], preds_lstm)\n",
    "print(f\"‚úÖ LSTM Accuracy: {acc_lstm:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üî• 3. PhoBERT (Optimized: Max Len 512 & Save Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Training PhoBERT (Optimized)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üåÄ Epoch 1/5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/11519 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (512) must match the existing size (258) at non-singleton dimension 1.  Target sizes: [8, 512].  Tensor sizes: [1, 258]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     42\u001b[39m labels = batch[\u001b[33m'\u001b[39m\u001b[33mlabels\u001b[39m\u001b[33m'\u001b[39m].to(device)\n\u001b[32m     44\u001b[39m opt.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m outputs = \u001b[43mmodel_bert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     46\u001b[39m outputs.loss.backward(); opt.step()\n\u001b[32m     47\u001b[39m progress_bar.set_postfix({\u001b[33m'\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m'\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutputs.loss.item()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m})\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\trading\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1780\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1779\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1780\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\trading\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1791\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1786\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1787\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1789\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1790\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1791\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1793\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1794\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\trading\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:1188\u001b[39m, in \u001b[36mRobertaForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1171\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1172\u001b[39m \u001b[33;03mtoken_type_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1173\u001b[39m \u001b[33;03m    Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,1]`:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1184\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m   1185\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1186\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m-> \u001b[39m\u001b[32m1188\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1199\u001b[39m sequence_output = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1200\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.classifier(sequence_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\trading\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1780\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1778\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1779\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1780\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\trading\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1791\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1786\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1787\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1788\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1789\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1790\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1791\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1793\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1794\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ASUS\\anaconda3\\envs\\trading\\Lib\\site-packages\\transformers\\models\\roberta\\modeling_roberta.py:793\u001b[39m, in \u001b[36mRobertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[39m\n\u001b[32m    791\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.embeddings, \u001b[33m\"\u001b[39m\u001b[33mtoken_type_ids\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    792\u001b[39m     buffered_token_type_ids = \u001b[38;5;28mself\u001b[39m.embeddings.token_type_ids[:, :seq_length]\n\u001b[32m--> \u001b[39m\u001b[32m793\u001b[39m     buffered_token_type_ids_expanded = \u001b[43mbuffered_token_type_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    794\u001b[39m     token_type_ids = buffered_token_type_ids_expanded\n\u001b[32m    795\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: The expanded size of the tensor (512) must match the existing size (258) at non-singleton dimension 1.  Target sizes: [8, 512].  Tensor sizes: [1, 258]"
     ]
    }
   ],
   "source": [
    "# --- 3. PHOBERT (FIXED & OPTIMIZED FOR 256 TOKENS) ---\n",
    "import torch\n",
    "import shutil\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"‚è≥ Training PhoBERT (Phi√™n b·∫£n V2 - MaxLen 256)...\")\n",
    "\n",
    "# --- C·∫§U H√åNH CHU·∫®N CHO PHOBERT ---\n",
    "# PhoBERT ch·ªâ h·ªó tr·ª£ t·ªëi ƒëa 256 token. Kh√¥ng ƒë∆∞·ª£c set cao h∆°n.\n",
    "MAX_LEN_BERT = 256  \n",
    "BATCH_SIZE = 32      # TƒÉng l√™n 32 v√¨ 256 token t·ªën √≠t VRAM h∆°n\n",
    "LEARNING_RATE = 2e-5 # LR chu·∫©n cho PhoBERT\n",
    "EPOCHS = 5           \n",
    "\n",
    "# Folder l∆∞u model t·ªët nh·∫•t\n",
    "PHOBERT_DIR = MODEL_DIR / \"phobert_best\"\n",
    "PHOBERT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Dataset Class\n",
    "class PhoBERTDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.texts = df['raw_text'].tolist()\n",
    "        self.labels = df['label_id'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = 256 # Gi·ªõi h·∫°n c·ªßa model\n",
    "\n",
    "    def __len__(self): return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        \n",
    "        # 1. Tokenize to√†n b·ªô b√†i (kh√¥ng c·∫Øt v·ªôi)\n",
    "        tokens = self.tokenizer.encode(text, add_special_tokens=True)\n",
    "        \n",
    "        # 2. X·ª≠ l√Ω HEAD + TAIL (Chi·∫øn thu·∫≠t l·∫≠t k√®o)\n",
    "        if len(tokens) > self.max_len:\n",
    "            # L·∫•y 128 token ƒë·∫ßu v√† 128 token cu·ªëi (Tr·ª´ 2 token ƒë·∫∑c bi·ªát CLS v√† SEP)\n",
    "            head_len = 128\n",
    "            tail_len = self.max_len - head_len\n",
    "            \n",
    "            # Gh√©p ƒë·∫ßu + ƒëu√¥i\n",
    "            input_ids = tokens[:head_len] + tokens[-tail_len:]\n",
    "        else:\n",
    "            # N·∫øu ng·∫Øn th√¨ pad th√™m s·ªë 0 cho ƒë·ªß\n",
    "            padding_len = self.max_len - len(tokens)\n",
    "            input_ids = tokens + [self.tokenizer.pad_token_id] * padding_len\n",
    "            \n",
    "        # Chuy·ªÉn th√†nh Tensor\n",
    "        input_ids = torch.tensor(input_ids)\n",
    "        attention_mask = (input_ids != self.tokenizer.pad_token_id).long()\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': torch.tensor(self.labels[idx])\n",
    "        }\n",
    "\n",
    "train_loader_bert = DataLoader(PhoBERTDataset(train_df), batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader_bert = DataLoader(PhoBERTDataset(test_df), batch_size=BATCH_SIZE)\n",
    "\n",
    "# Kh·ªüi t·∫°o Model (D√πng b·∫£n V2 x·ªãn h∆°n)\n",
    "MODEL_NAME = \"vinai/phobert-base-v2\" \n",
    "print(f\"   ‚û§ ƒêang t·∫£i model: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model_bert = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_classes).to(device)\n",
    "opt = AdamW(model_bert.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# --- V√íNG L·∫∂P TRAIN SAVE BEST ---\n",
    "best_acc = 0.0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"\\nüåÄ Epoch {epoch+1}/{EPOCHS}:\")\n",
    "    \n",
    "    # 1. Training\n",
    "    model_bert.train()\n",
    "    progress_bar = tqdm(train_loader_bert, desc=\"Training\")\n",
    "    \n",
    "    for batch in progress_bar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        opt.zero_grad()\n",
    "        outputs = model_bert(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        \n",
    "        progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    # 2. Evaluation\n",
    "    model_bert.eval()\n",
    "    preds_temp, targets_temp = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader_bert, desc=\"Evaluating\"):\n",
    "            outputs = model_bert(batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device))\n",
    "            preds_temp.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "            targets_temp.extend(batch['labels'].numpy())\n",
    "    \n",
    "    current_acc = accuracy_score(targets_temp, preds_temp)\n",
    "    print(f\"   üìà Accuracy: {current_acc:.4f}\")\n",
    "    \n",
    "    # 3. Save Best\n",
    "    if current_acc > best_acc:\n",
    "        print(f\"   üî• K·ª∑ l·ª•c m·ªõi! (Old: {best_acc:.4f} -> New: {current_acc:.4f})\")\n",
    "        model_bert.save_pretrained(PHOBERT_DIR)\n",
    "        tokenizer.save_pretrained(PHOBERT_DIR)\n",
    "        best_acc = current_acc\n",
    "    else:\n",
    "        print(f\"   ‚ö†Ô∏è Kh√¥ng v∆∞·ª£t qua k·ª∑ l·ª•c ({best_acc:.4f})\")\n",
    "\n",
    "print(f\"\\nüèÜ Model t·ªët nh·∫•t ƒë·∫°t Accuracy: {best_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç 4. Analysis & Report (Load Best Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load l·∫°i Model t·ªët nh·∫•t ƒë·ªÉ ph√¢n t√≠ch\n",
    "print(\"‚è≥ Loading Best PhoBERT for Analysis...\")\n",
    "best_model = AutoModelForSequenceClassification.from_pretrained(PHOBERT_DIR).to(device)\n",
    "best_model.eval()\n",
    "\n",
    "final_preds, final_targets = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader_bert, desc=\"Final Inference\"):\n",
    "        outputs = best_model(batch['input_ids'].to(device), attention_mask=batch['attention_mask'].to(device))\n",
    "        final_preds.extend(torch.argmax(outputs.logits, dim=1).cpu().numpy())\n",
    "        final_targets.extend(batch['labels'].numpy())\n",
    "\n",
    "# 1. Confusion Matrix\n",
    "plt.figure(figsize=(12, 10))\n",
    "cm = confusion_matrix(final_targets, final_preds)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "plt.title(f'Confusion Matrix (PhoBERT Best - Acc: {best_acc:.4f})')\n",
    "plt.xticks(rotation=45, ha='right'); plt.show()\n",
    "\n",
    "# 2. Classification Report\n",
    "print(\"\\nüìä Detailed Report:\")\n",
    "print(classification_report(final_targets, final_preds, target_names=classes))\n",
    "\n",
    "# 3. Soi m·∫´u sai\n",
    "print(\"\\nüßê C√ÅC M·∫™U SAI ƒêI·ªÇN H√åNH:\")\n",
    "wrong_idx = [i for i, (p, t) in enumerate(zip(final_preds, final_targets)) if p != t]\n",
    "import random\n",
    "if wrong_idx:\n",
    "    for idx in random.sample(wrong_idx, min(5, len(wrong_idx))):\n",
    "        print(\"-\"*80)\n",
    "        print(f\"üî¥ Th·ª±c t·∫ø: {classes[final_targets[idx]]} | üîµ D·ª± ƒëo√°n: {classes[final_preds[idx]]}\")\n",
    "        # Hack ƒë·ªÉ l·∫•y text g·ªëc t·ª´ dataset th√¥ng qua index (l∆∞u √Ω: test_loader kh√¥ng shuffle n√™n index kh·ªõp v·ªõi test_df)\n",
    "        print(f\"üìñ {test_df.iloc[idx]['raw_text'][:200]}...\")\n",
    "\n",
    "# 4. Save Final Report\n",
    "results = pd.DataFrame([\n",
    "    {\"Model\": \"SVM\", \"Accuracy\": acc_svm},\n",
    "    {\"Model\": \"Logistic Regression\", \"Accuracy\": acc_lr},\n",
    "    {\"Model\": \"Naive Bayes\", \"Accuracy\": acc_nb},\n",
    "    {\"Model\": \"LSTM\", \"Accuracy\": acc_lstm},\n",
    "    {\"Model\": \"PhoBERT (Best)\", \"Accuracy\": best_acc}\n",
    "]).sort_values(by=\"Accuracy\", ascending=False)\n",
    "display(results)\n",
    "results.to_excel(REPORT_DIR / \"final_leaderboard.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. SAVE ALL MODELS ---\n",
    "print(\"üíæ ƒêang l∆∞u c√°c model c√≤n l·∫°i...\")\n",
    "# Sklearn\n",
    "joblib.dump(le, MODEL_DIR / \"label_encoder.pkl\")\n",
    "joblib.dump(tfidf, MODEL_DIR / \"tfidf_vectorizer.pkl\")\n",
    "joblib.dump(svm, MODEL_DIR / \"svm_linear.pkl\")\n",
    "joblib.dump(lr, MODEL_DIR / \"logistic_regression.pkl\")\n",
    "\n",
    "# LSTM\n",
    "lstm_checkpoint = {\n",
    "    'vocab': vocab, 'model_state': model_lstm.state_dict(),\n",
    "    'config': {'vocab_size': len(vocab)+2, 'embed_dim': 100, 'hidden_dim': 100, 'num_classes': num_classes, 'max_len': MAX_LEN_LSTM}\n",
    "}\n",
    "torch.save(lstm_checkpoint, MODEL_DIR / \"lstm_model.pth\")\n",
    "print(\"‚úÖ All Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (trading)",
   "language": "python",
   "name": "trading"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
