import streamlit as st
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import joblib
import unicodedata
import requests
from bs4 import BeautifulSoup
from pyvi import ViTokenizer
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import io

# --- Cáº¤U HÃŒNH TRANG ---
st.set_page_config(
    page_title="My Reading Trends",
    page_icon="eye",
    layout="wide"
)

# --- Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN ---
BASE_DIR = Path(__file__).parent
MODEL_DIR = BASE_DIR / "models"
if not MODEL_DIR.exists(): MODEL_DIR = BASE_DIR

# --- 1. KHá»I Táº O SESSION STATE (Bá»˜ NHá»š Táº M) ---
if 'history' not in st.session_state:
    st.session_state['history'] = []  # List chá»©a cÃ¡c bÃ i Ä‘Ã£ phÃ¢n tÃ­ch

# --- 2. Äá»ŠNH NGHÄ¨A MODEL & Xá»¬ LÃ (GIá»® NGUYÃŠN Tá»ª TRÆ¯á»šC) ---
class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, num_classes)
    def forward(self, x):
        embedded = self.embedding(x)
        output, (h_n, c_n) = self.lstm(embedded)
        last_hidden = h_n[-1]; out = self.fc(last_hidden); return out

STOPWORD_PATH = BASE_DIR / "data" / "final" / "vietnamese-stopwords-dash.txt"

def load_stopwords(filepath):
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return set([line.strip() for line in f.readlines()])
    except FileNotFoundError:
        print(f"Lá»—i: KhÃ´ng tÃ¬m tháº¥y file stopwords táº¡i {filepath}")
        return {"thÃ¬", "lÃ ", "mÃ "}

STOPWORDS = load_stopwords(STOPWORD_PATH)

def normalize_text(text): return unicodedata.normalize('NFC', text)

def preprocess_text(text):
    text = normalize_text(text)
    tokenized = ViTokenizer.tokenize(text)
    words = tokenized.split()
    clean_words = [w for w in words if w.lower() not in STOPWORDS and len(w) > 1]
    return " ".join(clean_words)

def crawl_news_from_url(url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/91.0.4472.124 Safari/537.36'}
    try:
        response = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Logic láº¥y ná»™i dung chÃ­nh (má»Ÿ rá»™ng thÃªm cÃ¡c bÃ¡o khÃ¡c á»Ÿ Ä‘Ã¢y)
        content = ""
        title = "BÃ i viáº¿t tá»« Link"
        
        # TiÃªu Ä‘á»
        if soup.title: title = soup.title.string
        
        # Ná»™i dung (Thá»­ cÃ¡c class phá»• biáº¿n)
        paragraphs = soup.find_all('p', class_=['Normal', 'description', 'content'])
        if not paragraphs: paragraphs = soup.find_all('p') # Fallback láº¥y táº¥t cáº£ tháº» p
        
        content = "\n".join([p.text.strip() for p in paragraphs if len(p.text.strip()) > 50])
        
        if len(content) < 100: return None, None, "Ná»™i dung quÃ¡ ngáº¯n hoáº·c khÃ´ng crawl Ä‘Æ°á»£c."
        return title, content, None
    except Exception as e: return None, None, str(e)

@st.cache_resource
def load_models():
    try:
        le = joblib.load(MODEL_DIR / "label_encoder.pkl")
        tfidf = joblib.load(MODEL_DIR / "tfidf_vectorizer.pkl")
        lr = joblib.load(MODEL_DIR / "logistic_regression.pkl")
        return le, tfidf, lr
    except: return None, None, None

le, tfidf, model = load_models()

# --- 3. GIAO DIá»†N CHÃNH ---

# Sidebar: NÃºt Reset
with st.sidebar:
    st.title("âš™ï¸ CÃ i Ä‘áº·t")
    if st.button("ğŸ—‘ï¸ XÃ³a lá»‹ch sá»­", type="primary"):
        st.session_state['history'] = []
        st.rerun()
    st.info("Há»‡ thá»‘ng sáº½ tÃ­ch lÅ©y cÃ¡c bÃ i báº¡n nháº­p vÃ o Ä‘á»ƒ phÃ¢n tÃ­ch xu hÆ°á»›ng Ä‘á»c.")

st.title("ğŸ“Š Personal Content Analyzer")
st.markdown("Há»‡ thá»‘ng phÃ¢n tÃ­ch xu hÆ°á»›ng ná»™i dung ngÆ°á»i dÃ¹ng (User Profiling).")

if not le:
    st.error("âŒ Thiáº¿u model. Vui lÃ²ng kiá»ƒm tra folder 'models'."); st.stop()

# --- KHU Vá»°C 1: NHáº¬P LIá»†U (ADD TO LIST) ---
with st.container(border=True):
    st.subheader("â• ThÃªm ná»™i dung má»›i")
    
    # DÃ¹ng tabs cho gá»n
    tab_link, tab_text, tab_file = st.tabs(["ğŸ”— Nháº­p Link", "ğŸ“ Nháº­p VÄƒn báº£n", "ğŸ“‚ Upload File (.txt)"])
    
    input_data = None
    input_type = None
    input_title = None # TÃªn hiá»ƒn thá»‹ trong lá»‹ch sá»­
    
    # 1. Xá»­ lÃ½ Tab Link
    with tab_link:
        url = st.text_input("DÃ¡n Ä‘Æ°á»ng dáº«n bÃ i bÃ¡o:", placeholder="https://...")
        if st.button("PhÃ¢n tÃ­ch Link"):
            if url:
                with st.spinner("Äang Ä‘á»c ná»™i dung tá»« web..."):
                    title, content, err = crawl_news_from_url(url)
                    if err: st.error(err)
                    else:
                        input_data = content
                        input_type = "Link"
                        input_title = title

    # 2. Xá»­ lÃ½ Tab Text
    with tab_text:
        txt = st.text_area("DÃ¡n ná»™i dung vÃ o Ä‘Ã¢y:", height=100)
        if st.button("PhÃ¢n tÃ­ch VÄƒn báº£n"):
            if txt:
                input_data = txt
                input_type = "Text"
                input_title = f"VÄƒn báº£n ({txt[:30]}...)"

    # 3. Xá»­ lÃ½ Tab File
    with tab_file:
        uploaded_file = st.file_uploader("Chá»n file .txt", type="txt")
        if uploaded_file is not None and st.button("PhÃ¢n tÃ­ch File"):
            stringio = io.StringIO(uploaded_file.getvalue().decode("utf-8"))
            content = stringio.read()
            if content:
                input_data = content
                input_type = "File"
                input_title = uploaded_file.name

    # --- CORE: Xá»¬ LÃ & LÆ¯U VÃ€O SESSION STATE ---
    if input_data:
        # 1. Preprocess
        clean_text = preprocess_text(input_data)
        
        # 2. Predict
        vec = tfidf.transform([clean_text])
        probs = model.predict_proba(vec)[0]
        pred_idx = np.argmax(probs)
        label = le.inverse_transform([pred_idx])[0]
        conf = probs[pred_idx]
        
        # 3. LÆ°u vÃ o lá»‹ch sá»­
        new_entry = {
            "title": input_title,
            "type": input_type,
            "topic": label,
            "confidence": conf,
            "preview": input_data[:100] + "..."
        }
        st.session_state['history'].append(new_entry)
        st.success(f"ÄÃ£ thÃªm: **{label}** ({conf:.1%})")
        # KhÃ´ng rerun Ä‘á»ƒ ngÆ°á»i dÃ¹ng cÃ³ thá»ƒ nháº­p tiáº¿p liÃªn tá»¥c

# --- KHU Vá»°C 2: DASHBOARD XU HÆ¯á»šNG ---
st.divider()

if len(st.session_state['history']) > 0:
    st.subheader("ğŸ“ˆ Xu hÆ°á»›ng Ä‘á»c cá»§a báº¡n")
    
    # Chuyá»ƒn lá»‹ch sá»­ thÃ nh DataFrame Ä‘á»ƒ dá»… xá»­ lÃ½
    df_history = pd.DataFrame(st.session_state['history'])
    
    # 1. KPIs
    col1, col2, col3 = st.columns(3)
    col1.metric("Tá»•ng sá»‘ bÃ i Ä‘Ã£ Ä‘á»c", len(df_history))
    
    top_topic = df_history['topic'].mode()[0]
    col2.metric("Chá»§ Ä‘á» quan tÃ¢m nháº¥t", top_topic)
    
    avg_conf = df_history['confidence'].mean()
    col3.metric("Äá»™ tin cáº­y trung bÃ¬nh AI", f"{avg_conf:.1%}")
    
    # 2. Charts & Details
    c_chart, c_list = st.columns([1, 1])
    
    with c_chart:
        st.write("##### PhÃ¢n bá»‘ chá»§ Ä‘á»")
        # Váº½ Pie Chart
        topic_counts = df_history['topic'].value_counts()
        fig, ax = plt.subplots(figsize=(5, 5))
        colors = sns.color_palette('pastel')[0:len(topic_counts)]
        ax.pie(topic_counts, labels=topic_counts.index, autopct='%1.1f%%', colors=colors, startangle=90)
        st.pyplot(fig)

    with c_list:
        st.write("##### Lá»‹ch sá»­ chi tiáº¿t")
        # Hiá»ƒn thá»‹ dáº¡ng báº£ng rÃºt gá»n
        st.dataframe(
            df_history[['topic', 'title', 'type', 'confidence']].style.highlight_max(axis=0, subset=['confidence']),
            column_config={
                "topic": "Chá»§ Ä‘á»",
                "title": "Nguá»“n / TiÃªu Ä‘á»",
                "type": "Loáº¡i",
                "confidence": st.column_config.NumberColumn("Äá»™ tin cáº­y", format="%.2f")
            },
            use_container_width=True,
            height=300
        )

else:

    st.info("ğŸ‘ˆ HÃ£y nháº­p Link, VÄƒn báº£n hoáº·c File á»Ÿ trÃªn Ä‘á»ƒ xem Dashboard phÃ¢n tÃ­ch xu hÆ°á»›ng.")
