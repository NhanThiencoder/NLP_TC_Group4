import json
import os
from pathlib import Path

from natsort import natsorted
from tqdm import tqdm
import natsort

# ================= C·∫§U H√åNH =================
BASE_DIR = Path(__file__).resolve().parent.parent
DATA_DIR = BASE_DIR / "data" / "processed" / "data_filtered"
OUTPUT_FILE = BASE_DIR / "data" / "final" / "nlp_dataset.jsonl"
MAPPING_FILE = BASE_DIR / "data" / "final" / "id2label.json"


def create_dataset_jsonl():
    if not DATA_DIR.exists():
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c: {DATA_DIR}")
        return

    # 1. T·∫°o Mapping ID
    topics = natsorted([d.name for d in DATA_DIR.iterdir() if d.is_dir()])
    label2id = {name: idx for idx, name in enumerate(topics)}
    id2label = {idx: name for idx, name in enumerate(topics)}

    print(f"üìä T√¨m th·∫•y {len(topics)} ch·ªß ƒë·ªÅ.")

    # 2. L∆∞u file Mapping (ƒê·ªÉ sau n√†y bi·∫øt s·ªë 0 l√† topic g√¨)
    with open(MAPPING_FILE, 'w', encoding='utf-8') as f:
        json.dump(id2label, f, ensure_ascii=False, indent=4)
    print(f"‚úÖ ƒê√£ l∆∞u file mapping: {MAPPING_FILE.name}")

    # 3. Duy·ªát file v√† Ghi tr·ª±c ti·∫øp v√†o JSONL (Stream write)
    print(f"üöÄ ƒêang t·∫°o dataset {OUTPUT_FILE.name}...")

    total_files = sum(len(list(d.glob("*.txt"))) for d in DATA_DIR.iterdir() if d.is_dir())

    # M·ªü file dataset ƒë·ªÉ ghi d√≤ng (Append mode)
    with open(OUTPUT_FILE, 'w', encoding='utf-8') as out_f:
        with tqdm(total=total_files, unit="file") as pbar:
            for topic_name in topics:
                topic_dir = DATA_DIR / topic_name
                topic_id = label2id[topic_name]

                # L·∫•y file v√† sort
                files = sorted(topic_dir.glob("*.txt"))

                for file_path in files:
                    try:
                        content = file_path.read_text(encoding='utf-8', errors='ignore').strip()
                        if content:
                            # T·∫°o object
                            record = {
                                "text": content,
                                "label_name": topic_name,
                                "label_id": topic_id,
                                "filename": file_path.name  # L∆∞u th√™m t√™n file g·ªëc ƒë·ªÉ d·ªÖ trace
                            }
                            # Ghi ngay l·∫≠p t·ª©c 1 d√≤ng JSON v√†o file
                            out_f.write(json.dumps(record, ensure_ascii=False) + '\n')

                    except Exception as e:
                        print(f"[ERR] {file_path.name}: {e}")

                    pbar.update(1)

    print(f"\n‚úÖ HO√ÄN T·∫§T! File dataset ƒë√£ s·∫µn s√†ng ƒë·ªÉ train.")


if __name__ == "__main__":
    create_dataset_jsonl()