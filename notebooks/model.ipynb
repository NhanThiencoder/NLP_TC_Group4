{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üß† NLP Topic Classification Model (Stage B)\n",
    "\n",
    "This notebook handles the **Training Phase** of the pipeline:\n",
    "1.  **Load Data**: Reads the clean `nlp_dataset.jsonl` created in Stage A.\n",
    "2.  **Tokenization**: Segments Vietnamese words using `pyvi` (e.g., \"H√† N·ªôi\" -> \"H√†_N·ªôi\").\n",
    "3.  **Vectorization**: Converts text to numbers using **TF-IDF** (removing stopwords here).\n",
    "4.  **Modeling**: Trains a **Logistic Regression** classifier.\n",
    "5.  **Evaluation**: Checks Accuracy, F1-Score, and analyzes specific Error cases.\n",
    "6.  **Export**: Saves the trained model for deployment."
   ],
   "id": "7854ffed974ba05d"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T21:54:01.979715Z",
     "start_time": "2025-12-16T21:54:01.692480Z"
    }
   },
   "source": [
    "# --- 1. SETUP & IMPORTS ---\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from pyvi import ViTokenizer\n",
    "\n",
    "# Scikit-learn modules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Configuration\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "DATA_FILE = \"nlp_dataset.jsonl\"\n",
    "MODEL_PATH = \"nlp_topic_classifier.pkl\"\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ],
   "id": "8fbe4eee1b7df598",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data & Prepare Stopwords"
   ],
   "id": "85429c120ef5c012"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T21:54:30.626133Z",
     "start_time": "2025-12-16T21:54:24.850061Z"
    }
   },
   "source": [
    "# Load Dataset\n",
    "print(f\"üìÇ Loading dataset from {DATA_FILE}...\")\n",
    "\n",
    "try:\n",
    "    df = pd.read_json(DATA_FILE, lines=True)\n",
    "    print(f\"‚úÖ Loaded {len(df)} records.\")\n",
    "except ValueError:\n",
    "    print(\"‚ùå Error: Dataset file not found. Please run 'create_dataset.py' first.\")\n",
    "\n",
    "# Define Vietnamese Stopwords\n",
    "# These words are grammatical glue that don't carry much topic meaning\n",
    "STOPWORDS = {\n",
    "    \"c·ªßa\", \"v√†\", \"c√°c\", \"c√≥\", \"ƒë∆∞·ª£c\", \"cho\", \"trong\", \"m·ªôt\", \"l√†\", \"v·ªõi\", \"ƒë·ªÉ\", \n",
    "    \"nh·ªØng\", \"khi\", \"n√†y\", \"khi\", \"t·∫°i\", \"ƒë√£\", \"th√¨\", \"m√†\", \"nh∆∞\", \"nƒÉm\", \"t·ª´\",\n",
    "    \"ƒë·∫øn\", \"ng∆∞·ªùi\", \"s·∫Ω\", \"c≈©ng\", \"v·ªÅ\", \"v√†o\", \"ra\", \"n√™n\", \"n·∫øu\", \"b·ªã\", \"b·ªüi\",\n",
    "    \"l·∫°i\", \"do\", \"nh∆∞ng\", \"ƒëang\", \"v·∫´n\", \"ch·ªâ\", \"theo\", \"g√¨\", \"ai\", \"ƒë√¢u\", \"r·∫±ng\"\n",
    "}\n",
    "STOPWORDS_LIST = list(STOPWORDS)\n",
    "\n",
    "# Preview data\n",
    "df[['label_name', 'text']].head(3)"
   ],
   "id": "245d29817ba940ae",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Loading dataset from nlp_dataset.jsonl...\n",
      "‚úÖ Loaded 126687 records.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "  label_name  \\\n",
       "0    ·∫®m th·ª±c   \n",
       "1    ·∫®m th·ª±c   \n",
       "2    ·∫®m th·ª±c   \n",
       "\n",
       "                                                                                                                                                                                                      text  \n",
       "0  M·ªôt trong nh·ªØng ph√¢n khu s√¥i ƒë·ªông nh·∫•t H·ªôi ch·ª£ M√πa Thu 2025 ƒëang di·ªÖn ra t·∫°i Trung t√¢m H·ªôi ch·ª£ Tri·ªÉn l√£m Qu·ªëc gia (·ªü x√£ ƒê√¥ng Anh, H√† N·ªôi) l√† ‚ÄúThu m·ªπ v·ªã‚Äù. Khu v·ª±c n√†y ƒëem ƒë·∫øn m·ªôt ƒë·∫°i ti·ªác ·∫©m th·ª±c t...  \n",
       "1  Ch·ªß ƒë·ªÅ v·ªÅ ·∫©m th·ª±c H√† th√†nh lu√¥n thu h√∫t s·ª± quan t√¢m c·ªßa nhi·ªÅu ng∆∞·ªùi. Sau khi D√¢n tr√≠ ƒëƒÉng t·∫£i b√†i vi·∫øt Tranh c√£i m√¢m c·ªó ngh·ªá nh√¢n ·ªü B√°t Tr√†ng 6 ng∆∞·ªùi gi√° 2,7 tri·ªáu ƒë·ªìng ƒë·∫Øt ƒë·ªè, ƒë·ªôc gi·∫£ v√† b·∫°n ƒë·ªçc ...  \n",
       "2  T·ª± l√†m b√°nh m·ª≥, b√°nh ph·ªü, nh·∫≠p m·∫Øm t√¥m t·ª´ Vi·ªát Nam sang M·ªπ\\n\\nTrong b√†i vi·∫øt ƒë∆∞·ª£c t√°c gi·∫£ Helen Rosner chia s·∫ª nh·∫≠n ƒë·ªãnh tr√™n t·ªù New Yorker, m·ªôt nh√† h√†ng do ng∆∞·ªùi Vi·ªát Nam l√†m ch·ªß ·ªü khu Upper West...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_name</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>·∫®m th·ª±c</td>\n",
       "      <td>M·ªôt trong nh·ªØng ph√¢n khu s√¥i ƒë·ªông nh·∫•t H·ªôi ch·ª£ M√πa Thu 2025 ƒëang di·ªÖn ra t·∫°i Trung t√¢m H·ªôi ch·ª£ Tri·ªÉn l√£m Qu·ªëc gia (·ªü x√£ ƒê√¥ng Anh, H√† N·ªôi) l√† ‚ÄúThu m·ªπ v·ªã‚Äù. Khu v·ª±c n√†y ƒëem ƒë·∫øn m·ªôt ƒë·∫°i ti·ªác ·∫©m th·ª±c t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>·∫®m th·ª±c</td>\n",
       "      <td>Ch·ªß ƒë·ªÅ v·ªÅ ·∫©m th·ª±c H√† th√†nh lu√¥n thu h√∫t s·ª± quan t√¢m c·ªßa nhi·ªÅu ng∆∞·ªùi. Sau khi D√¢n tr√≠ ƒëƒÉng t·∫£i b√†i vi·∫øt Tranh c√£i m√¢m c·ªó ngh·ªá nh√¢n ·ªü B√°t Tr√†ng 6 ng∆∞·ªùi gi√° 2,7 tri·ªáu ƒë·ªìng ƒë·∫Øt ƒë·ªè, ƒë·ªôc gi·∫£ v√† b·∫°n ƒë·ªçc ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>·∫®m th·ª±c</td>\n",
       "      <td>T·ª± l√†m b√°nh m·ª≥, b√°nh ph·ªü, nh·∫≠p m·∫Øm t√¥m t·ª´ Vi·ªát Nam sang M·ªπ\\n\\nTrong b√†i vi·∫øt ƒë∆∞·ª£c t√°c gi·∫£ Helen Rosner chia s·∫ª nh·∫≠n ƒë·ªãnh tr√™n t·ªù New Yorker, m·ªôt nh√† h√†ng do ng∆∞·ªùi Vi·ªát Nam l√†m ch·ªß ·ªü khu Upper West...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Vietnamese Word Segmentation (T√°ch t·ª´)\n",
    "Using `pyvi` to join compound words with underscores (e.g., `h·ªçc sinh` -> `h·ªçc_sinh`). This is crucial for TF-IDF to treat them as single tokens."
   ],
   "id": "cbc72ae77693c925"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"‚öôÔ∏è Tokenizing text (This may take a minute)...\")\n",
    "\n",
    "def segment_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    return ViTokenizer.tokenize(text)\n",
    "\n",
    "# Apply tokenization to a new column\n",
    "df['segmented_text'] = df['text'].apply(segment_text)\n",
    "\n",
    "print(\"‚úÖ Tokenization Complete!\")\n",
    "print(\"Sample Original:  \", df['text'].iloc[0][:60])\n",
    "print(\"Sample Segmented: \", df['segmented_text'].iloc[0][:60])"
   ],
   "id": "a3cddfc6159a5bb7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train - Test Split\n",
    "We split the data **BEFORE** vectorization to prevent data leakage."
   ],
   "id": "a840b7e21ecef27a"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
    "    df['segmented_text'], \n",
    "    df['label_id'], \n",
    "    df.index, # Keep index to trace back filenames later\n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=df['label_id'] # Ensure balanced classes in test set\n",
    ")\n",
    "\n",
    "print(f\"üîπ Training Set: {len(X_train)} samples\")\n",
    "print(f\"üîπ Test Set:     {len(X_test)} samples\")"
   ],
   "id": "b0794f6d0e0dd8ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Build & Train Pipeline\n",
    "We use a `Pipeline` to bundle:\n",
    "1.  **TfidfVectorizer**: Converts text to vectors, filtering stopwords and rare words.\n",
    "2.  **LogisticRegression**: A fast and effective baseline model for text classification."
   ],
   "id": "fb68a2fcadd91ab9"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"üöÄ Training Model...\")\n",
    "\n",
    "model_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        max_features=20000,      # Keep top 20k important words\n",
    "        ngram_range=(1, 2),      # Use unigrams and bigrams\n",
    "        stop_words=STOPWORDS_LIST,\n",
    "        min_df=2                 # Ignore words appearing < 2 times\n",
    "    )),\n",
    "    ('clf', LogisticRegression(\n",
    "        solver='sag',            # Optimized for large datasets\n",
    "        multi_class='multinomial', \n",
    "        max_iter=1000,\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "\n",
    "print(\"‚úÖ Training Finished.\")"
   ],
   "id": "b3ea73ca69b729f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation\n",
    "Assessing model performance on the Test set."
   ],
   "id": "6926baa3720e139d"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Predictions\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "\n",
    "# Get correct label names map\n",
    "label_map = df.drop_duplicates('label_id').sort_values('label_id')\n",
    "label_names = label_map['label_name'].tolist()\n",
    "\n",
    "# Metrics\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"üèÜ Accuracy: {acc:.2%}\\n\")\n",
    "\n",
    "print(\"üìä Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_names))"
   ],
   "id": "cc82d1bcf7bc692a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Visualization"
   ],
   "id": "3766300e62ae933c"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=label_names, \n",
    "            yticklabels=label_names)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ],
   "id": "57c1547bb2327faf",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Error Analysis (Quan tr·ªçng)\n",
    "Identify which specific files confused the model. This is possible because we preserved the `filename`."
   ],
   "id": "12ba79f37f1ae81f"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Reconstruct Test DataFrame with predictions\n",
    "test_results = df.loc[idx_test].copy()\n",
    "test_results['predicted_id'] = y_pred\n",
    "test_results['predicted_label'] = test_results['predicted_id'].apply(lambda x: label_names[x])\n",
    "\n",
    "# Filter Errors\n",
    "errors = test_results[test_results['label_id'] != test_results['predicted_id']]\n",
    "\n",
    "print(f\"‚ö†Ô∏è Total Misclassified: {len(errors)} / {len(X_test)}\")\n",
    "print(\"samples of errors:\")\n",
    "\n",
    "# Show random 5 errors\n",
    "if not errors.empty:\n",
    "    cols_to_show = ['filename', 'label_name', 'predicted_label', 'text']\n",
    "    display(errors[cols_to_show].sample(min(5, len(errors))))\n",
    "else:\n",
    "    print(\"Wow! No errors found. Perfect model? Check for overfitting.\")"
   ],
   "id": "8e83f7a1aec2c742",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Model & Live Inference"
   ],
   "id": "bde53e378ce31255"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save pipeline\n",
    "joblib.dump(model_pipeline, MODEL_PATH)\n",
    "print(f\"üíæ Model saved to: {MODEL_PATH}\")\n",
    "\n",
    "# --- Inference Function ---\n",
    "def predict_news(text):\n",
    "    # Preprocessing (must match training)\n",
    "    tokenized = ViTokenizer.tokenize(text)\n",
    "    \n",
    "    # Predict\n",
    "    pred_id = model_pipeline.predict([tokenized])[0]\n",
    "    prob = model_pipeline.predict_proba([tokenized]).max()\n",
    "    \n",
    "    return label_names[pred_id], prob\n",
    "\n",
    "# Try it out\n",
    "sample_news = \"Gi√° chung c∆∞ t·∫°i H√† N·ªôi ti·∫øp t·ª•c l·∫≠p ƒë·ªânh m·ªõi do ngu·ªìn cung khan hi·∫øm.\"\n",
    "topic, conf = predict_news(sample_news)\n",
    "\n",
    "print(f\"\\nüì∞ Input: {sample_news}\")\n",
    "print(f\"ü§ñ Prediction: {topic} (Confidence: {conf:.1%})\")"
   ],
   "id": "5c37c3f85d0c5b8a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
